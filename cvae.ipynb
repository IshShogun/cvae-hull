{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "470b36e2-1b4a-4d9b-80be-4107567d0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CVAE_LSTM(nn.Module):\n",
    "    def __init__(self, iv_dim, feature_dim, hidden_dim, latent_dim, num_layers, dropout_rate):\n",
    "        input_dim = iv_dim + feature_dim\n",
    "        super(CVAE_LSTM, self).__init__() ##call the constructor of the pytorch nn module for the nn architecture\n",
    "        self.iv_dim = iv_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate #probability a neuron wont activate/be dropped out - prevents overfitting and coadaption btw nn\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        ##we define an an lstm layer for the encoder consuming the ts data and providing a hidden state\n",
    "        ##the mu and logvar layers provide nns to predict the parameters of the prob distribution for the latent represenation z given context x\n",
    "        ##these consume the hidden state to produce the predictions\n",
    "        self.encoder_lstm = nn.LSTM(iv_dim + 1, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate) ##only encode log returns for future\n",
    "        self.encoder_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.encoder_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Context Encoder\n",
    "        ##the encoder learns the probability distribution then when we have an data to encode we use this, maybe use the \n",
    "        self.context_encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate) #why only to lstm? - smth to write about\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(latent_dim + hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder_output = nn.Linear(hidden_dim, input_dim) #output is not input dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        #pass entire timeseries data into lstm for hidden state generation\n",
    "        _, (h_n, _) = self.encoder_lstm(x)\n",
    "        h_n = h_n.view(self.num_layers, -1, self.hidden_dim)[-1]\n",
    "        #then i need to generate a mu and logvar for each day\n",
    "        mu = self.encoder_mu(h_n)\n",
    "        logvar = self.encoder_logvar(h_n)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, context):\n",
    "        decoder_input = torch.cat((z, context), dim=-1)\n",
    "        output, _ = self.decoder_lstm(decoder_input)\n",
    "        output = self.decoder_output(output)\n",
    "        return output\n",
    "\n",
    "    def generate(self, z, x_c, y_c):\n",
    "        print('generate')\n",
    "        \n",
    "    def forward(self, x_c, y_c, x_n, r_n):\n",
    "        #generate a latent representation for each day z_bar -> then pass this in generate to produce future values (in generation steps after the model)\n",
    "        #will make it such that it takes k days in the past and predicts t days in the future. -> invariant x_c is of size 14 (14 days in the past)\n",
    "\n",
    "        #output will predict 3 days in the future (for now)\n",
    "        \n",
    "        #generate a latent representation for each day\n",
    "        #encode will generate a latent represenation z given context x by learning a probability distribution\n",
    "\n",
    "        #pass into generate to predict the 3 days in the future with context\n",
    "        \n",
    "        # Encode historical context\n",
    "        _, (context, _) = self.context_encoder_lstm(torch.cat((x_c, y_c), dim=-1))\n",
    "        context = context.view(self.num_layers, -1, self.hidden_dim)[-1]\n",
    "        \n",
    "        # Encode future values - will have a different dimensionality 25 + 1 as we only predict future log returns\n",
    "        mu, logvar = self.encode(torch.cat((x_n, r_n), dim=-1))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # Decode future values\n",
    "        x_hat, r_hat = self.decode(z, context).split(self.iv_dim, dim=-1)\n",
    "\n",
    "        return x_hat, r_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3914f2eb-7f10-4896-9685-6926e4e6edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3885, 25) (3885, 3)\n",
      "Validation set shape: (1000, 25) (1000, 3)\n",
      "Test set shape: (1000, 25) (1000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/_44jn7jx03b17y35qvlp17sr0000gn/T/ipykernel_51292/4159740756.py:13: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  X = df.drop(['Log Return', 'Skew', 'Slope'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the DataFrame from the provided CSV file\n",
    "file_path = 'combined_iv_data.csv'\n",
    "df = pd.read_csv(file_path, header=[0,1], index_col=0)\n",
    "\n",
    "#extra features will be these below\n",
    "Y = df[['Log Return', 'Skew', 'Slope']]\n",
    "\n",
    "# X will be all iv surface data - no features\n",
    "X = df.drop(['Log Return', 'Skew', 'Slope'], axis=1)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "Y_scaled = scaler.fit_transform(Y)\n",
    "\n",
    "# Split data into training and temporary set\n",
    "#tarining will have 4000 (saved 2000 for test and validation in temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, Y_scaled, test_size=2000, random_state=42)\n",
    "\n",
    "#X_train has all iv surface data to train with\n",
    "#y_train has all the extra features\n",
    "\n",
    "# Split the temporary set into validation and test sets, by splitting temp in half -> 1000 each\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Output the shapes of the splits to confirm the operation\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "771b67ba-a4d2-46ef-950a-a6cea1eb4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_day(n, arr):\n",
    "    \"\"\"\n",
    "    Generates a random number between 1 and n,\n",
    "    and checks if it is not already in the given array.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Generate a random number between 1 and n\n",
    "        random_number = random.randint(1, n - 1)\n",
    "        \n",
    "        # Check if the random number is not in the array\n",
    "        if random_number not in arr:\n",
    "            arr.append(random_number)\n",
    "            return random_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e3f8961-d55a-40a4-b2eb-1daf21b7ba4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 18 but got size 14 for tensor number 2 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#now from here reassing r_n to only have the log return feature - log returns at 0th index\u001b[39;00m\n\u001b[1;32m     60\u001b[0m r_n \u001b[38;5;241m=\u001b[39m r_n[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m x_hat, r_hat, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_hat shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_hat\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mx_n shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_n\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_hat shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr_hat\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mr_n shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr_n\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 105\u001b[0m, in \u001b[0;36mCVAE_LSTM.forward\u001b[0;34m(self, x_c, y_c, x_n, y_n)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_c, y_c, x_n, y_n):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Combine historical context and future values\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_n\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Pass the input through the encoder network\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     _, (h_n, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_lstm(input_data)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 14 for tensor number 2 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 5\n",
    "context_dim = 5\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "dropout_rate = 0.2\n",
    "kl_weight = 1e-5\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "val_data = TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val))\n",
    "test_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#input is historical iv and extra features\n",
    "model = CVAE_LSTM(iv_dim=(X_train.shape[-1]), feature_dim=(y_train.shape[-1]), hidden_dim=hidden_dim, latent_dim=latent_dim,\n",
    "                  num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) #step in right direction\n",
    "\n",
    "arr = []\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_surface_loss = 0.0\n",
    "    train_return_loss = 0.0\n",
    "    train_kl_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        #this gets us all iv surface and extra feature data for the batch\n",
    "        X_batch, Y_batch = batch\n",
    "\n",
    "        n = Y_batch.shape[0] #days to sample from\n",
    "        #generate a random t not already generated\n",
    "        t = generate_random_day(n, arr)\n",
    "\n",
    "        #up to t-1 is context from t -> end is future\n",
    "        x_c, x_n = X_batch[:t], X_batch[t:]\n",
    "        y_c, r_n = Y_batch[:t], Y_batch[t:] #need only the asset returns here\n",
    "\n",
    "        #now from here reassing r_n to only have the log return feature - log returns at 0th index\n",
    "        r_n = r_n[:, 0].unsqueeze(1)\n",
    "    \n",
    "        x_hat, r_hat, mu, logvar = model(x_c, y_c, x_n, r_n)\n",
    "\n",
    "        r_hat = r_hat[:, 0].unsqueeze(1)\n",
    "        \n",
    "        print(f\"x_hat shape {x_hat.shape}\\nx_n shape:{x_n.shape}\\n\\n\")\n",
    "        print(f\"r_hat shape {r_hat.shape}\\nr_n shape:{r_n.shape}\\n\\n\")\n",
    "        surface_loss = criterion(x_hat, x_n)\n",
    "        return_loss = criterion(r_hat, r_n)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        loss = surface_loss + return_loss + kl_weight * kl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_surface_loss += surface_loss.item()\n",
    "        train_return_loss += return_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        print(f\"IV Surface Loss: {surface_loss.item()}\")\n",
    "        print(f\"Log Return Loss: {return_loss.item()}\")\n",
    "        print(f\"KL Loss: {kl_loss.item()}\\n\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_surface_loss = 0.0\n",
    "    val_return_loss = 0.0\n",
    "    val_kl_loss = 0.0\n",
    "    arr = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            #this gets us all iv surface and extra feature data for the batch\n",
    "            X_batch, Y_batch = batch\n",
    "    \n",
    "            n = Y_batch.shape[0] #days to sample from\n",
    "            #generate a random t not already generated\n",
    "            t = generate_random_day(n, arr)\n",
    "    \n",
    "            #up to t-1 is context from t -> end is future\n",
    "            x_c, x_n = X_batch[:t], X_batch[t:]\n",
    "            y_c, r_n = Y_batch[:t], Y_batch[t:] #need only the asset returns here\n",
    "    \n",
    "            #now from here reassing r_n to only have the log return feature - log returns at 0th index\n",
    "            r_n = r_n[:, 0].unsqueeze(1)\n",
    "            \n",
    "            x_hat, r_hat, mu, logvar = model(x_c, y_c, x_n, r_n)\n",
    "\n",
    "            surface_loss = criterion(x_hat, x_n)\n",
    "            return_loss = criterion(r_hat, r_n)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            val_surface_loss += surface_loss.item()\n",
    "            val_return_loss += return_loss.item()\n",
    "            val_kl_loss += kl_loss.item()\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Surface Loss: {train_surface_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train Return Loss: {train_return_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train KL Loss: {train_kl_loss / len(train_loader):.4f}, \"\n",
    "          f\"Val Surface Loss: {val_surface_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val Return Loss: {val_return_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val KL Loss: {val_kl_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    val_loss = val_surface_loss + val_return_loss + kl_weight * val_kl_loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_surface_loss = 0.0\n",
    "test_return_loss = 0.0\n",
    "test_kl_loss = 0.0\n",
    "arr = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        #this gets us all iv surface and extra feature data for the batch\n",
    "        X_batch, Y_batch = batch\n",
    "\n",
    "        n = Y_batch.shape[0] #days to sample from\n",
    "        #generate a random t not already generated\n",
    "        t = generate_random_day(n, arr)\n",
    "\n",
    "        #up to t-1 is context from t -> end is future\n",
    "        x_c, x_n = X_batch[:t], X_batch[t:]\n",
    "        y_c, r_n = Y_batch[:t], Y_batch[t:] #need only the asset returns here\n",
    "\n",
    "        #now from here reassing r_n to only have the log return feature - log returns at 0th index\n",
    "        r_n = r_n[:, 0].unsqueeze(1)\n",
    "        x_hat, r_hat, mu, logvar = model(x_c, y_c, x_n, r_n)\n",
    "\n",
    "        surface_loss = criterion(x_hat, x_n)\n",
    "        return_loss = criterion(r_hat, r_n)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        test_surface_loss += surface_loss.item()\n",
    "        test_return_loss += return_loss.item()\n",
    "        test_kl_loss += kl_loss.item()\n",
    "\n",
    "print(f\"Test Surface Loss: {test_surface_loss / len(test_loader):.4f}, \"\n",
    "      f\"Test Return Loss: {test_return_loss / len(test_loader):.4f}, \"\n",
    "      f\"Test KL Loss: {test_kl_loss / len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaec439-b0aa-44d0-a7f9-5dae1b77aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##first the encoder takes the historical context with the future values to generate a distribution for each latent variable on each day\n",
    "##during training the latent variables is sampled from cooresponding distribution during generation sample from standard normal\n",
    "\n",
    "\n",
    "##context encoder encodes context\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
