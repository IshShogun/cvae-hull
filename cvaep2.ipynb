{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96622b7-9564-4921-a263-ba72644bbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "##The encoder in the paper takes x in R^(TxHxW) and y in R^(TxE) and maps them to R^(TxL) the dimensionality of the context encoder\n",
    "##and encoder is set to 5. So this cnn needs to output (32x5) so each (surface in R^(5x5) -> (z in R^5)\n",
    "\n",
    "##we enhance the dimensionality of the iv surface first by upgrading the number of channels. The reasoning behind this is similar to why \n",
    "##we do this in transformer architecture. A larger dimensional space will be able to capture more nuanced information and represent it in number form\n",
    "##then we compress this to something digestable\n",
    "class CNN(nn.Module):\n",
    "    #input_size and output_size represent the number of channels in the input and output data\n",
    "    #channels is the number of dimensions a single data point will have ie RGB = 3 channelss\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(output_size * 5 * 5, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, H, W = x.shape\n",
    "        x = x.reshape(batch_size, 1, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(f\"x shape before resize and convultion passthrough is {x.shape}\")\n",
    "        #x = x.reshape(batch_size, 1, 5)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        #print(f\"after passthrough into convultion layers and fully connected layer ther shape of x is {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = F.relu(self.fc1(y))\n",
    "        x = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377a416f-980c-4024-bb50-115fb221343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##had to make another cnn for decoder due difference in dimensionality\n",
    "\n",
    "##in here the output size should be the number of days in the future?\n",
    "\n",
    "##for now output_size = 1 so we predict 1 day into the future?\n",
    "class TCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_surfaces):\n",
    "        super(TCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5 * 5 * num_surfaces)\n",
    "        self.output_size = output_size\n",
    "        self.num_surfaces = num_surfaces\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, self.num_surfaces, 5, 5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3016d2ec-e952-4aa8-ad75-2cf4b43b5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cnn = CNN(input_size, 5)\n",
    "        self.mlp = nn.Identity()\n",
    "        self.lstm = nn.LSTM(5 + 3, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.linear_sigma = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_encoded = self.cnn(x)\n",
    "        y_encoded = self.mlp(y)\n",
    "        y_encoded = torch.squeeze(y_encoded, dim=1)\n",
    "        #print(f\"x_encoded shape is {x_encoded.shape}  y_encoded shape is {y_encoded.shape}\")\n",
    "        encoded = torch.cat((x_encoded, y_encoded), dim=-1)\n",
    "        #print(f\"concatenated vector is of size {encoded.shape}\")\n",
    "        _, (hidden, _) = self.lstm(encoded)\n",
    "        #print(\"hidden state created\")\n",
    "        hidden = hidden[-1]  # Take the last hidden state\n",
    "        mu = self.linear_mu(hidden)\n",
    "        log_var = self.linear_sigma(hidden)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        #print(\"encoding successful\")\n",
    "        return z, mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, context_size):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.cnn = CNN(input_size, 5)\n",
    "        self.mlp = nn.Identity()\n",
    "        self.lstm = nn.LSTM(5 + 3, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, context_size)\n",
    "\n",
    "    def forward(self, x_c, y_c):\n",
    "        x_encoded = self.cnn(x_c)\n",
    "        y_encoded = self.mlp(y_c)\n",
    "        y_encoded = torch.squeeze(y_encoded, dim=1)\n",
    "        #print(f\"x_encoded size = {x_encoded.shape} ||y_encoded size = {y_encoded.shape}\")\n",
    "        encoded = torch.cat((x_encoded, y_encoded), dim=-1)\n",
    "        _, (hidden, _) = self.lstm(encoded)\n",
    "        hidden = hidden[-1]  # Take the last hidden state\n",
    "        zeta = self.linear(hidden)\n",
    "        #print(\"context encoding successful\")\n",
    "        return zeta\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_surfaces):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_size + 5, hidden_size, num_layers=2, batch_first=True, dropout=0.2) #latent\n",
    "        self.tcnn = TCNN(hidden_size, output_size, num_surfaces)\n",
    "        self.mlp = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, z, zeta):\n",
    "        # Reshape z and zeta to have shape (1, 5)\n",
    "        z = z.view(1, -1)\n",
    "        zeta = zeta.view(1, -1)\n",
    "        # Concatenate z and zeta along the second dimension to get shape (1, 10)\n",
    "        z_concat = torch.cat((z, zeta), dim=1)\n",
    "        \n",
    "        #print(f\"z_concat is of size {z_concat.shape}\")\n",
    "        hidden, _ = self.lstm(z_concat)\n",
    "        #print('i am here')\n",
    "        #print(f\"hidden state published, hidden state shape {hidden.shape}\")\n",
    "        x_n = self.tcnn(hidden) \n",
    "        #print('i am here')\n",
    "        r_n = self.mlp(hidden)\n",
    "        #print(\"decoding successful\")\n",
    "        return torch.squeeze(x_n, dim=0), torch.squeeze(r_n, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fe4a5a-20e5-4168-bd9c-03ab231b460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, context_size, output_size, num_surfaces):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, latent_size)\n",
    "        self.context_encoder = ContextEncoder(input_size, hidden_size, context_size)\n",
    "        self.decoder = Decoder(latent_size, hidden_size, output_size, num_surfaces)\n",
    "        self.latent_size = latent_size\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def forward(self, x, y, x_c, y_c):\n",
    "        #print('1')\n",
    "        z, mu, log_var = self.encoder(x, y) ##we have sampled z from distribution here\n",
    "        #print(f\"The shape of latent representation z is {z.shape}\")\n",
    "        #print('2')\n",
    "        zeta = self.context_encoder(x_c, y_c) ##we have sampled zeta from distribution here\n",
    "        #print(f\"the shape of zeta (encoded context) is {zeta.shape}\")\n",
    "        x_n, r_n = self.decoder(z, zeta)\n",
    "        return x_n, r_n, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3be940-4695-424d-8c99-f1964625f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/_44jn7jx03b17y35qvlp17sr0000gn/T/ipykernel_75151/3904911613.py:14: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  X = df.drop(['Log Return', 'Skew', 'Slope'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the DataFrame from the provided CSV file\n",
    "file_path = 'combined_iv_data.csv'\n",
    "df = pd.read_csv(file_path, header=[0, 1], index_col=0)\n",
    "\n",
    "# Extract extra features\n",
    "Y = df[['Log Return', 'Skew', 'Slope']]\n",
    "\n",
    "# Extract IV surface data\n",
    "X = df.drop(['Log Return', 'Skew', 'Slope'], axis=1)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "Y_scaled = scaler.fit_transform(Y)\n",
    "\n",
    "# Split data into training and temporary set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, Y_scaled, test_size=2000, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a68456d-e530-49b0-847a-d1ea1f5f2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_day(n):\n",
    "    \"\"\"\n",
    "    Generates a random number between 1 and n\n",
    "    \"\"\"\n",
    "    # Generate a random number between 1 and n\n",
    "    random_number = random.randint(1, n - 1)\n",
    "    return random_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34bc80fd-0266-41e6-87fe-8b89a2370bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch_dnnl (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch_dnnl\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_dnnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73cfc4fe-13dd-4d85-b490-ab4eb4620f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/_44jn7jx03b17y35qvlp17sr0000gn/T/ipykernel_75151/1260078627.py:59: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss_r = F.mse_loss(r_n_recon, r_n, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 1.0306, Val Loss: 1.0999\n",
      "Epoch [2/500], Train Loss: 0.8024, Val Loss: 1.0828\n",
      "Epoch [3/500], Train Loss: 0.6992, Val Loss: 0.8012\n",
      "Epoch [4/500], Train Loss: 0.5023, Val Loss: 0.3835\n",
      "Epoch [5/500], Train Loss: 0.2391, Val Loss: 0.2193\n",
      "Epoch [6/500], Train Loss: 0.1572, Val Loss: 0.2410\n",
      "Epoch [7/500], Train Loss: 0.1415, Val Loss: 0.1208\n",
      "Epoch [8/500], Train Loss: 0.1221, Val Loss: 0.0818\n",
      "Epoch [9/500], Train Loss: 0.0918, Val Loss: 0.1231\n",
      "Epoch [10/500], Train Loss: 0.1166, Val Loss: 0.1708\n",
      "Epoch [11/500], Train Loss: 0.1445, Val Loss: 0.1065\n",
      "Epoch [12/500], Train Loss: 0.0927, Val Loss: 0.1217\n",
      "Epoch [13/500], Train Loss: 0.0833, Val Loss: 0.1114\n",
      "Epoch [14/500], Train Loss: 0.0922, Val Loss: 0.0848\n",
      "Epoch [15/500], Train Loss: 0.1320, Val Loss: 0.1056\n",
      "Epoch [16/500], Train Loss: 0.0867, Val Loss: 0.1424\n",
      "Epoch [17/500], Train Loss: 0.0804, Val Loss: 0.0719\n",
      "Epoch [18/500], Train Loss: 0.0625, Val Loss: 0.0724\n",
      "Epoch [19/500], Train Loss: 0.0985, Val Loss: 0.0600\n",
      "Epoch [20/500], Train Loss: 0.0635, Val Loss: 0.0815\n",
      "Epoch [21/500], Train Loss: 0.0613, Val Loss: 0.0472\n",
      "Epoch [22/500], Train Loss: 0.0882, Val Loss: 0.0907\n",
      "Epoch [23/500], Train Loss: 0.1149, Val Loss: 0.0903\n",
      "Epoch [24/500], Train Loss: 0.0749, Val Loss: 0.0725\n",
      "Epoch [25/500], Train Loss: 0.0852, Val Loss: 0.0820\n",
      "Epoch [26/500], Train Loss: 0.0820, Val Loss: 0.0648\n",
      "Epoch [27/500], Train Loss: 0.0714, Val Loss: 0.0622\n",
      "Epoch [28/500], Train Loss: 0.1127, Val Loss: 0.1369\n",
      "Epoch [29/500], Train Loss: 0.1186, Val Loss: 0.0593\n",
      "Epoch [30/500], Train Loss: 0.0818, Val Loss: 0.0661\n",
      "Epoch [31/500], Train Loss: 0.1124, Val Loss: 0.0709\n",
      "Epoch [32/500], Train Loss: 0.0746, Val Loss: 0.0436\n",
      "Epoch [33/500], Train Loss: 0.0593, Val Loss: 0.0500\n",
      "Epoch [34/500], Train Loss: 0.0621, Val Loss: 0.0475\n",
      "Epoch [35/500], Train Loss: 0.0608, Val Loss: 0.0433\n",
      "Epoch [36/500], Train Loss: 0.0690, Val Loss: 0.0564\n",
      "Epoch [37/500], Train Loss: 0.0647, Val Loss: 0.0653\n",
      "Epoch [38/500], Train Loss: 0.0878, Val Loss: 0.1272\n",
      "Epoch [39/500], Train Loss: 0.1162, Val Loss: 0.0604\n",
      "Epoch [40/500], Train Loss: 0.0708, Val Loss: 0.0742\n",
      "Epoch [41/500], Train Loss: 0.0684, Val Loss: 0.0571\n",
      "Epoch [42/500], Train Loss: 0.0725, Val Loss: 0.0388\n",
      "Epoch [43/500], Train Loss: 0.0750, Val Loss: 0.0912\n",
      "Epoch [44/500], Train Loss: 0.0775, Val Loss: 0.0639\n",
      "Epoch [45/500], Train Loss: 0.0684, Val Loss: 0.0523\n",
      "Epoch [46/500], Train Loss: 0.0632, Val Loss: 0.0750\n",
      "Epoch [47/500], Train Loss: 0.0656, Val Loss: 0.0611\n",
      "Epoch [48/500], Train Loss: 0.0634, Val Loss: 0.0545\n",
      "Epoch [49/500], Train Loss: 0.0672, Val Loss: 0.0418\n",
      "Epoch [50/500], Train Loss: 0.0658, Val Loss: 0.0476\n",
      "Epoch [51/500], Train Loss: 0.0721, Val Loss: 0.0466\n",
      "Epoch [52/500], Train Loss: 0.0672, Val Loss: 0.0558\n",
      "Epoch [53/500], Train Loss: 0.0803, Val Loss: 0.0519\n",
      "Epoch [54/500], Train Loss: 0.0665, Val Loss: 0.0542\n",
      "Epoch [55/500], Train Loss: 0.0606, Val Loss: 0.0632\n",
      "Epoch [56/500], Train Loss: 0.0881, Val Loss: 0.0602\n",
      "Epoch [57/500], Train Loss: 0.0736, Val Loss: 0.0662\n",
      "Epoch [58/500], Train Loss: 0.0611, Val Loss: 0.0665\n",
      "Epoch [59/500], Train Loss: 0.0630, Val Loss: 0.0566\n",
      "Epoch [60/500], Train Loss: 0.0749, Val Loss: 0.0617\n",
      "Epoch [61/500], Train Loss: 0.0678, Val Loss: 0.0510\n",
      "Epoch [62/500], Train Loss: 0.0595, Val Loss: 0.0487\n",
      "Epoch [63/500], Train Loss: 0.0624, Val Loss: 0.0389\n",
      "Epoch [64/500], Train Loss: 0.0541, Val Loss: 0.0715\n",
      "Epoch [65/500], Train Loss: 0.0677, Val Loss: 0.0592\n",
      "Epoch [66/500], Train Loss: 0.0756, Val Loss: 0.1184\n",
      "Epoch [67/500], Train Loss: 0.0593, Val Loss: 0.0544\n",
      "Epoch [68/500], Train Loss: 0.0579, Val Loss: 0.0444\n",
      "Epoch [69/500], Train Loss: 0.0630, Val Loss: 0.0552\n",
      "Epoch [70/500], Train Loss: 0.0630, Val Loss: 0.0428\n",
      "Epoch [71/500], Train Loss: 0.0724, Val Loss: 0.0502\n",
      "Epoch [72/500], Train Loss: 0.0579, Val Loss: 0.0476\n",
      "Epoch [73/500], Train Loss: 0.0624, Val Loss: 0.0461\n",
      "Epoch [74/500], Train Loss: 0.0607, Val Loss: 0.0553\n",
      "Epoch [75/500], Train Loss: 0.0581, Val Loss: 0.0384\n",
      "Epoch [76/500], Train Loss: 0.0723, Val Loss: 0.0413\n",
      "Epoch [77/500], Train Loss: 0.0668, Val Loss: 0.0448\n",
      "Epoch [78/500], Train Loss: 0.0556, Val Loss: 0.0462\n",
      "Epoch [79/500], Train Loss: 0.0540, Val Loss: 0.0505\n",
      "Epoch [80/500], Train Loss: 0.0613, Val Loss: 0.0399\n",
      "Epoch [81/500], Train Loss: 0.0532, Val Loss: 0.0515\n",
      "Epoch [82/500], Train Loss: 0.0640, Val Loss: 0.0566\n",
      "Epoch [83/500], Train Loss: 0.0734, Val Loss: 0.1221\n",
      "Epoch [84/500], Train Loss: 0.0710, Val Loss: 0.0671\n",
      "Epoch [85/500], Train Loss: 0.0653, Val Loss: 0.0569\n",
      "Epoch [86/500], Train Loss: 0.0779, Val Loss: 0.0635\n",
      "Epoch [87/500], Train Loss: 0.0569, Val Loss: 0.0575\n",
      "Epoch [88/500], Train Loss: 0.0574, Val Loss: 0.0400\n",
      "Epoch [89/500], Train Loss: 0.0670, Val Loss: 0.0661\n",
      "Epoch [90/500], Train Loss: 0.0746, Val Loss: 0.0444\n",
      "Epoch [91/500], Train Loss: 0.0613, Val Loss: 0.0634\n",
      "Epoch [92/500], Train Loss: 0.0525, Val Loss: 0.0649\n",
      "Epoch [93/500], Train Loss: 0.0596, Val Loss: 0.0435\n",
      "Epoch [94/500], Train Loss: 0.0529, Val Loss: 0.0519\n",
      "Epoch [95/500], Train Loss: 0.0609, Val Loss: 0.0404\n",
      "Epoch [96/500], Train Loss: 0.0525, Val Loss: 0.0466\n",
      "Epoch [97/500], Train Loss: 0.0675, Val Loss: 0.0437\n",
      "Epoch [98/500], Train Loss: 0.0643, Val Loss: 0.0371\n",
      "Epoch [99/500], Train Loss: 0.0546, Val Loss: 0.0531\n",
      "Epoch [100/500], Train Loss: 0.0797, Val Loss: 0.0528\n",
      "Epoch [101/500], Train Loss: 0.0618, Val Loss: 0.0599\n",
      "Epoch [102/500], Train Loss: 0.0699, Val Loss: 0.0633\n",
      "Epoch [103/500], Train Loss: 0.0727, Val Loss: 0.0438\n",
      "Epoch [104/500], Train Loss: 0.0709, Val Loss: 0.0522\n",
      "Epoch [105/500], Train Loss: 0.0566, Val Loss: 0.0344\n",
      "Epoch [106/500], Train Loss: 0.0483, Val Loss: 0.0443\n",
      "Epoch [107/500], Train Loss: 0.0577, Val Loss: 0.0340\n",
      "Epoch [108/500], Train Loss: 0.0535, Val Loss: 0.0521\n",
      "Epoch [109/500], Train Loss: 0.0530, Val Loss: 0.0434\n",
      "Epoch [110/500], Train Loss: 0.0567, Val Loss: 0.0442\n",
      "Epoch [111/500], Train Loss: 0.0502, Val Loss: 0.0406\n",
      "Epoch [112/500], Train Loss: 0.0545, Val Loss: 0.0517\n",
      "Epoch [113/500], Train Loss: 0.0498, Val Loss: 0.0341\n",
      "Epoch [114/500], Train Loss: 0.0533, Val Loss: 0.0675\n",
      "Epoch [115/500], Train Loss: 0.0575, Val Loss: 0.0394\n",
      "Epoch [116/500], Train Loss: 0.0612, Val Loss: 0.0590\n",
      "Epoch [117/500], Train Loss: 0.0521, Val Loss: 0.0409\n",
      "Epoch [118/500], Train Loss: 0.0557, Val Loss: 0.0418\n",
      "Epoch [119/500], Train Loss: 0.0730, Val Loss: 0.0545\n",
      "Epoch [120/500], Train Loss: 0.0647, Val Loss: 0.0405\n",
      "Epoch [121/500], Train Loss: 0.0635, Val Loss: 0.0537\n",
      "Epoch [122/500], Train Loss: 0.0672, Val Loss: 0.0480\n",
      "Epoch [123/500], Train Loss: 0.0563, Val Loss: 0.0362\n",
      "Epoch [124/500], Train Loss: 0.0568, Val Loss: 0.0366\n",
      "Epoch [125/500], Train Loss: 0.0612, Val Loss: 0.0428\n",
      "Epoch [126/500], Train Loss: 0.0541, Val Loss: 0.0725\n",
      "Epoch [127/500], Train Loss: 0.0539, Val Loss: 0.0602\n",
      "Epoch [128/500], Train Loss: 0.0536, Val Loss: 0.0570\n",
      "Epoch [129/500], Train Loss: 0.0747, Val Loss: 0.0496\n",
      "Epoch [130/500], Train Loss: 0.0716, Val Loss: 0.0664\n",
      "Epoch [131/500], Train Loss: 0.0654, Val Loss: 0.0608\n",
      "Epoch [132/500], Train Loss: 0.0613, Val Loss: 0.0437\n",
      "Epoch [133/500], Train Loss: 0.0583, Val Loss: 0.0662\n",
      "Epoch [134/500], Train Loss: 0.0666, Val Loss: 0.0380\n",
      "Epoch [135/500], Train Loss: 0.0692, Val Loss: 0.0406\n",
      "Epoch [136/500], Train Loss: 0.0604, Val Loss: 0.0500\n",
      "Epoch [137/500], Train Loss: 0.0518, Val Loss: 0.0525\n",
      "Epoch [138/500], Train Loss: 0.0514, Val Loss: 0.0498\n",
      "Epoch [139/500], Train Loss: 0.0683, Val Loss: 0.0852\n",
      "Epoch [140/500], Train Loss: 0.0686, Val Loss: 0.0423\n",
      "Epoch [141/500], Train Loss: 0.0583, Val Loss: 0.0394\n",
      "Epoch [142/500], Train Loss: 0.0591, Val Loss: 0.0534\n",
      "Epoch [143/500], Train Loss: 0.0688, Val Loss: 0.0536\n",
      "Epoch [144/500], Train Loss: 0.0486, Val Loss: 0.0482\n",
      "Epoch [145/500], Train Loss: 0.0526, Val Loss: 0.0409\n",
      "Epoch [146/500], Train Loss: 0.0618, Val Loss: 0.0471\n",
      "Epoch [147/500], Train Loss: 0.0632, Val Loss: 0.0391\n",
      "Epoch [148/500], Train Loss: 0.0699, Val Loss: 0.0394\n",
      "Epoch [149/500], Train Loss: 0.0640, Val Loss: 0.0464\n",
      "Epoch [150/500], Train Loss: 0.0460, Val Loss: 0.0405\n",
      "Epoch [151/500], Train Loss: 0.0577, Val Loss: 0.0460\n",
      "Epoch [152/500], Train Loss: 0.0570, Val Loss: 0.0378\n",
      "Epoch [153/500], Train Loss: 0.0679, Val Loss: 0.0623\n",
      "Epoch [154/500], Train Loss: 0.0631, Val Loss: 0.0443\n",
      "Epoch [155/500], Train Loss: 0.0513, Val Loss: 0.0406\n",
      "Epoch [156/500], Train Loss: 0.0604, Val Loss: 0.0567\n",
      "Epoch [157/500], Train Loss: 0.0529, Val Loss: 0.0792\n",
      "Epoch [158/500], Train Loss: 0.0532, Val Loss: 0.0672\n",
      "Epoch [159/500], Train Loss: 0.0638, Val Loss: 0.0511\n",
      "Epoch [160/500], Train Loss: 0.0593, Val Loss: 0.0433\n",
      "Epoch [161/500], Train Loss: 0.0570, Val Loss: 0.0466\n",
      "Epoch [162/500], Train Loss: 0.0625, Val Loss: 0.0432\n",
      "Epoch [163/500], Train Loss: 0.0571, Val Loss: 0.0547\n",
      "Epoch [164/500], Train Loss: 0.0589, Val Loss: 0.0629\n",
      "Epoch [165/500], Train Loss: 0.0513, Val Loss: 0.0652\n",
      "Epoch [166/500], Train Loss: 0.0510, Val Loss: 0.0515\n",
      "Epoch [167/500], Train Loss: 0.0655, Val Loss: 0.0665\n",
      "Epoch [168/500], Train Loss: 0.0478, Val Loss: 0.0454\n",
      "Epoch [169/500], Train Loss: 0.0498, Val Loss: 0.0553\n",
      "Epoch [170/500], Train Loss: 0.0572, Val Loss: 0.0434\n",
      "Epoch [171/500], Train Loss: 0.0612, Val Loss: 0.0616\n",
      "Epoch [172/500], Train Loss: 0.0637, Val Loss: 0.0449\n",
      "Epoch [173/500], Train Loss: 0.0500, Val Loss: 0.0464\n",
      "Epoch [174/500], Train Loss: 0.0548, Val Loss: 0.0415\n",
      "Epoch [175/500], Train Loss: 0.0545, Val Loss: 0.0410\n",
      "Epoch [176/500], Train Loss: 0.0496, Val Loss: 0.0454\n",
      "Epoch [177/500], Train Loss: 0.0597, Val Loss: 0.0384\n",
      "Epoch [178/500], Train Loss: 0.0704, Val Loss: 0.0604\n",
      "Epoch [179/500], Train Loss: 0.0749, Val Loss: 0.0424\n",
      "Epoch [180/500], Train Loss: 0.0591, Val Loss: 0.1103\n",
      "Epoch [181/500], Train Loss: 0.0589, Val Loss: 0.0434\n",
      "Epoch [182/500], Train Loss: 0.0580, Val Loss: 0.0388\n",
      "Epoch [183/500], Train Loss: 0.0479, Val Loss: 0.0379\n",
      "Epoch [184/500], Train Loss: 0.0525, Val Loss: 0.0450\n",
      "Epoch [185/500], Train Loss: 0.0489, Val Loss: 0.0517\n",
      "Epoch [186/500], Train Loss: 0.0624, Val Loss: 0.0487\n",
      "Epoch [187/500], Train Loss: 0.0599, Val Loss: 0.0535\n",
      "Epoch [188/500], Train Loss: 0.0638, Val Loss: 0.0486\n",
      "Epoch [189/500], Train Loss: 0.0501, Val Loss: 0.0502\n",
      "Epoch [190/500], Train Loss: 0.0518, Val Loss: 0.0619\n",
      "Epoch [191/500], Train Loss: 0.0564, Val Loss: 0.0634\n",
      "Epoch [192/500], Train Loss: 0.0561, Val Loss: 0.0597\n",
      "Epoch [193/500], Train Loss: 0.0665, Val Loss: 0.0603\n",
      "Epoch [194/500], Train Loss: 0.0548, Val Loss: 0.0518\n",
      "Epoch [195/500], Train Loss: 0.0557, Val Loss: 0.0404\n",
      "Epoch [196/500], Train Loss: 0.0587, Val Loss: 0.0447\n",
      "Epoch [197/500], Train Loss: 0.0526, Val Loss: 0.0401\n",
      "Epoch [198/500], Train Loss: 0.0788, Val Loss: 0.0585\n",
      "Epoch [199/500], Train Loss: 0.0801, Val Loss: 0.0425\n",
      "Epoch [200/500], Train Loss: 0.0557, Val Loss: 0.0388\n",
      "Epoch [201/500], Train Loss: 0.0677, Val Loss: 0.0506\n",
      "Epoch [202/500], Train Loss: 0.0500, Val Loss: 0.0361\n",
      "Epoch [203/500], Train Loss: 0.0485, Val Loss: 0.0362\n",
      "Epoch [204/500], Train Loss: 0.0568, Val Loss: 0.0539\n",
      "Epoch [205/500], Train Loss: 0.0652, Val Loss: 0.0389\n",
      "Epoch [206/500], Train Loss: 0.0498, Val Loss: 0.0381\n",
      "Epoch [207/500], Train Loss: 0.0672, Val Loss: 0.0728\n",
      "Epoch [208/500], Train Loss: 0.0596, Val Loss: 0.0396\n",
      "Epoch [209/500], Train Loss: 0.0506, Val Loss: 0.0395\n",
      "Epoch [210/500], Train Loss: 0.0776, Val Loss: 0.0425\n",
      "Epoch [211/500], Train Loss: 0.0590, Val Loss: 0.0364\n",
      "Epoch [212/500], Train Loss: 0.0532, Val Loss: 0.0517\n",
      "Epoch [213/500], Train Loss: 0.0686, Val Loss: 0.0372\n",
      "Epoch [214/500], Train Loss: 0.0555, Val Loss: 0.0577\n",
      "Epoch [215/500], Train Loss: 0.0558, Val Loss: 0.0366\n",
      "Epoch [216/500], Train Loss: 0.0549, Val Loss: 0.0389\n",
      "Epoch [217/500], Train Loss: 0.0603, Val Loss: 0.0370\n",
      "Epoch [218/500], Train Loss: 0.0500, Val Loss: 0.0418\n",
      "Epoch [219/500], Train Loss: 0.0509, Val Loss: 0.0355\n",
      "Epoch [220/500], Train Loss: 0.0468, Val Loss: 0.0461\n",
      "Epoch [221/500], Train Loss: 0.0601, Val Loss: 0.0526\n",
      "Epoch [222/500], Train Loss: 0.0645, Val Loss: 0.0495\n",
      "Epoch [223/500], Train Loss: 0.0519, Val Loss: 0.0493\n",
      "Epoch [224/500], Train Loss: 0.0533, Val Loss: 0.0512\n",
      "Epoch [225/500], Train Loss: 0.0620, Val Loss: 0.0442\n",
      "Epoch [226/500], Train Loss: 0.0566, Val Loss: 0.0388\n",
      "Epoch [227/500], Train Loss: 0.0541, Val Loss: 0.0516\n",
      "Epoch [228/500], Train Loss: 0.0497, Val Loss: 0.0423\n",
      "Epoch [229/500], Train Loss: 0.0590, Val Loss: 0.0448\n",
      "Epoch [230/500], Train Loss: 0.0579, Val Loss: 0.0341\n",
      "Epoch [231/500], Train Loss: 0.0592, Val Loss: 0.0411\n",
      "Epoch [232/500], Train Loss: 0.0557, Val Loss: 0.0447\n",
      "Epoch [233/500], Train Loss: 0.0447, Val Loss: 0.0503\n",
      "Epoch [234/500], Train Loss: 0.0546, Val Loss: 0.0511\n",
      "Epoch [235/500], Train Loss: 0.0598, Val Loss: 0.0427\n",
      "Epoch [236/500], Train Loss: 0.0548, Val Loss: 0.0430\n",
      "Epoch [237/500], Train Loss: 0.0504, Val Loss: 0.0379\n",
      "Epoch [238/500], Train Loss: 0.0552, Val Loss: 0.0383\n",
      "Epoch [239/500], Train Loss: 0.0489, Val Loss: 0.0399\n",
      "Epoch [240/500], Train Loss: 0.0532, Val Loss: 0.0429\n",
      "Epoch [241/500], Train Loss: 0.0521, Val Loss: 0.0393\n",
      "Epoch [242/500], Train Loss: 0.0542, Val Loss: 0.0435\n",
      "Epoch [243/500], Train Loss: 0.0674, Val Loss: 0.0518\n",
      "Epoch [244/500], Train Loss: 0.0873, Val Loss: 0.0992\n",
      "Epoch [245/500], Train Loss: 0.0612, Val Loss: 0.0450\n",
      "Epoch [246/500], Train Loss: 0.0557, Val Loss: 0.0361\n",
      "Epoch [247/500], Train Loss: 0.0467, Val Loss: 0.0493\n",
      "Epoch [248/500], Train Loss: 0.0487, Val Loss: 0.0395\n",
      "Epoch [249/500], Train Loss: 0.0476, Val Loss: 0.0401\n",
      "Epoch [250/500], Train Loss: 0.0588, Val Loss: 0.0648\n",
      "Epoch [251/500], Train Loss: 0.0505, Val Loss: 0.0394\n",
      "Epoch [252/500], Train Loss: 0.0547, Val Loss: 0.0382\n",
      "Epoch [253/500], Train Loss: 0.0457, Val Loss: 0.0377\n",
      "Epoch [254/500], Train Loss: 0.0507, Val Loss: 0.0854\n",
      "Epoch [255/500], Train Loss: 0.0501, Val Loss: 0.0391\n",
      "Epoch [256/500], Train Loss: 0.0823, Val Loss: 0.0423\n",
      "Epoch [257/500], Train Loss: 0.0635, Val Loss: 0.0508\n",
      "Epoch [258/500], Train Loss: 0.0613, Val Loss: 0.0464\n",
      "Epoch [259/500], Train Loss: 0.0557, Val Loss: 0.0348\n",
      "Epoch [260/500], Train Loss: 0.0581, Val Loss: 0.0370\n",
      "Epoch [261/500], Train Loss: 0.0478, Val Loss: 0.0389\n",
      "Epoch [262/500], Train Loss: 0.0455, Val Loss: 0.0415\n",
      "Epoch [263/500], Train Loss: 0.0520, Val Loss: 0.0377\n",
      "Epoch [264/500], Train Loss: 0.0496, Val Loss: 0.0433\n",
      "Epoch [265/500], Train Loss: 0.0446, Val Loss: 0.0380\n",
      "Epoch [266/500], Train Loss: 0.0688, Val Loss: 0.0366\n",
      "Epoch [267/500], Train Loss: 0.0573, Val Loss: 0.0357\n",
      "Epoch [268/500], Train Loss: 0.0498, Val Loss: 0.0411\n",
      "Epoch [269/500], Train Loss: 0.0581, Val Loss: 0.0360\n",
      "Epoch [270/500], Train Loss: 0.0499, Val Loss: 0.0368\n",
      "Epoch [271/500], Train Loss: 0.0485, Val Loss: 0.0444\n",
      "Epoch [272/500], Train Loss: 0.0389, Val Loss: 0.0407\n",
      "Epoch [273/500], Train Loss: 0.0589, Val Loss: 0.0612\n",
      "Epoch [274/500], Train Loss: 0.0526, Val Loss: 0.0414\n",
      "Epoch [275/500], Train Loss: 0.0613, Val Loss: 0.0467\n",
      "Epoch [276/500], Train Loss: 0.0604, Val Loss: 0.0450\n",
      "Epoch [277/500], Train Loss: 0.0626, Val Loss: 0.0714\n",
      "Epoch [278/500], Train Loss: 0.0649, Val Loss: 0.0609\n",
      "Epoch [279/500], Train Loss: 0.0589, Val Loss: 0.0389\n",
      "Epoch [280/500], Train Loss: 0.0514, Val Loss: 0.0546\n",
      "Epoch [281/500], Train Loss: 0.0639, Val Loss: 0.0369\n",
      "Epoch [282/500], Train Loss: 0.0492, Val Loss: 0.0347\n",
      "Epoch [283/500], Train Loss: 0.0503, Val Loss: 0.0377\n",
      "Epoch [284/500], Train Loss: 0.0524, Val Loss: 0.0379\n",
      "Epoch [285/500], Train Loss: 0.0517, Val Loss: 0.0363\n",
      "Epoch [286/500], Train Loss: 0.0583, Val Loss: 0.0482\n",
      "Epoch [287/500], Train Loss: 0.0702, Val Loss: 0.0510\n",
      "Epoch [288/500], Train Loss: 0.0503, Val Loss: 0.0401\n",
      "Epoch [289/500], Train Loss: 0.0477, Val Loss: 0.0428\n",
      "Epoch [290/500], Train Loss: 0.0700, Val Loss: 0.0517\n",
      "Epoch [291/500], Train Loss: 0.0686, Val Loss: 0.0387\n",
      "Epoch [292/500], Train Loss: 0.0642, Val Loss: 0.0365\n",
      "Epoch [293/500], Train Loss: 0.0492, Val Loss: 0.0383\n",
      "Epoch [294/500], Train Loss: 0.0497, Val Loss: 0.0376\n",
      "Epoch [295/500], Train Loss: 0.0460, Val Loss: 0.0382\n",
      "Epoch [296/500], Train Loss: 0.0520, Val Loss: 0.0449\n",
      "Epoch [297/500], Train Loss: 0.0593, Val Loss: 0.0368\n",
      "Epoch [298/500], Train Loss: 0.0536, Val Loss: 0.0339\n",
      "Epoch [299/500], Train Loss: 0.0478, Val Loss: 0.0413\n",
      "Epoch [300/500], Train Loss: 0.0435, Val Loss: 0.0400\n",
      "Epoch [301/500], Train Loss: 0.0564, Val Loss: 0.0513\n",
      "Epoch [302/500], Train Loss: 0.0568, Val Loss: 0.0573\n",
      "Epoch [303/500], Train Loss: 0.0478, Val Loss: 0.0419\n",
      "Epoch [304/500], Train Loss: 0.0611, Val Loss: 0.0448\n",
      "Epoch [305/500], Train Loss: 0.0568, Val Loss: 0.0462\n",
      "Epoch [306/500], Train Loss: 0.0467, Val Loss: 0.0402\n",
      "Epoch [307/500], Train Loss: 0.0471, Val Loss: 0.0424\n",
      "Epoch [308/500], Train Loss: 0.0619, Val Loss: 0.0373\n",
      "Epoch [309/500], Train Loss: 0.0480, Val Loss: 0.0421\n",
      "Epoch [310/500], Train Loss: 0.0562, Val Loss: 0.0400\n",
      "Epoch [311/500], Train Loss: 0.0634, Val Loss: 0.0443\n",
      "Epoch [312/500], Train Loss: 0.0586, Val Loss: 0.0495\n",
      "Epoch [313/500], Train Loss: 0.0586, Val Loss: 0.0375\n",
      "Epoch [314/500], Train Loss: 0.0578, Val Loss: 0.0361\n",
      "Epoch [315/500], Train Loss: 0.0453, Val Loss: 0.0540\n",
      "Epoch [316/500], Train Loss: 0.0533, Val Loss: 0.0373\n",
      "Epoch [317/500], Train Loss: 0.0545, Val Loss: 0.0347\n",
      "Epoch [318/500], Train Loss: 0.0553, Val Loss: 0.0369\n",
      "Epoch [319/500], Train Loss: 0.0526, Val Loss: 0.0337\n",
      "Epoch [320/500], Train Loss: 0.0497, Val Loss: 0.0417\n",
      "Epoch [321/500], Train Loss: 0.0578, Val Loss: 0.0361\n",
      "Epoch [322/500], Train Loss: 0.0462, Val Loss: 0.0355\n",
      "Epoch [323/500], Train Loss: 0.0489, Val Loss: 0.0477\n",
      "Epoch [324/500], Train Loss: 0.0581, Val Loss: 0.0442\n",
      "Epoch [325/500], Train Loss: 0.0644, Val Loss: 0.0401\n",
      "Epoch [326/500], Train Loss: 0.0507, Val Loss: 0.0379\n",
      "Epoch [327/500], Train Loss: 0.0546, Val Loss: 0.0468\n",
      "Epoch [328/500], Train Loss: 0.0466, Val Loss: 0.0361\n",
      "Epoch [329/500], Train Loss: 0.0513, Val Loss: 0.0470\n",
      "Epoch [330/500], Train Loss: 0.0607, Val Loss: 0.0409\n",
      "Epoch [331/500], Train Loss: 0.0529, Val Loss: 0.0372\n",
      "Epoch [332/500], Train Loss: 0.0516, Val Loss: 0.0388\n",
      "Epoch [333/500], Train Loss: 0.0534, Val Loss: 0.0400\n",
      "Epoch [334/500], Train Loss: 0.0506, Val Loss: 0.0422\n",
      "Epoch [335/500], Train Loss: 0.0548, Val Loss: 0.0362\n",
      "Epoch [336/500], Train Loss: 0.0551, Val Loss: 0.0430\n",
      "Epoch [337/500], Train Loss: 0.0558, Val Loss: 0.0390\n",
      "Epoch [338/500], Train Loss: 0.0536, Val Loss: 0.0397\n",
      "Epoch [339/500], Train Loss: 0.0567, Val Loss: 0.0388\n",
      "Epoch [340/500], Train Loss: 0.0551, Val Loss: 0.0463\n",
      "Epoch [341/500], Train Loss: 0.0510, Val Loss: 0.0406\n",
      "Epoch [342/500], Train Loss: 0.0609, Val Loss: 0.0365\n",
      "Epoch [343/500], Train Loss: 0.0627, Val Loss: 0.0433\n",
      "Epoch [344/500], Train Loss: 0.0573, Val Loss: 0.0366\n",
      "Epoch [345/500], Train Loss: 0.0525, Val Loss: 0.0408\n",
      "Epoch [346/500], Train Loss: 0.0606, Val Loss: 0.0367\n",
      "Epoch [347/500], Train Loss: 0.0563, Val Loss: 0.0395\n",
      "Epoch [348/500], Train Loss: 0.0580, Val Loss: 0.0340\n",
      "Epoch [349/500], Train Loss: 0.0507, Val Loss: 0.0356\n",
      "Epoch [350/500], Train Loss: 0.0584, Val Loss: 0.0361\n",
      "Epoch [351/500], Train Loss: 0.0665, Val Loss: 0.0442\n",
      "Epoch [352/500], Train Loss: 0.0531, Val Loss: 0.0382\n",
      "Epoch [353/500], Train Loss: 0.0441, Val Loss: 0.0483\n",
      "Epoch [354/500], Train Loss: 0.0506, Val Loss: 0.0380\n",
      "Epoch [355/500], Train Loss: 0.0551, Val Loss: 0.0346\n",
      "Epoch [356/500], Train Loss: 0.0569, Val Loss: 0.0338\n",
      "Epoch [357/500], Train Loss: 0.0481, Val Loss: 0.0363\n",
      "Epoch [358/500], Train Loss: 0.0459, Val Loss: 0.0331\n",
      "Epoch [359/500], Train Loss: 0.0595, Val Loss: 0.0457\n",
      "Epoch [360/500], Train Loss: 0.0623, Val Loss: 0.0401\n",
      "Epoch [361/500], Train Loss: 0.0454, Val Loss: 0.0408\n",
      "Epoch [362/500], Train Loss: 0.0532, Val Loss: 0.0382\n",
      "Epoch [363/500], Train Loss: 0.0585, Val Loss: 0.0650\n",
      "Epoch [364/500], Train Loss: 0.0490, Val Loss: 0.0356\n",
      "Epoch [365/500], Train Loss: 0.0596, Val Loss: 0.0391\n",
      "Epoch [366/500], Train Loss: 0.0501, Val Loss: 0.0473\n",
      "Epoch [367/500], Train Loss: 0.0562, Val Loss: 0.0568\n",
      "Epoch [368/500], Train Loss: 0.0559, Val Loss: 0.0346\n",
      "Epoch [369/500], Train Loss: 0.0566, Val Loss: 0.0372\n",
      "Epoch [370/500], Train Loss: 0.0583, Val Loss: 0.0384\n",
      "Epoch [371/500], Train Loss: 0.0434, Val Loss: 0.0338\n",
      "Epoch [372/500], Train Loss: 0.0454, Val Loss: 0.0374\n",
      "Epoch [373/500], Train Loss: 0.0509, Val Loss: 0.0328\n",
      "Epoch [374/500], Train Loss: 0.0523, Val Loss: 0.0382\n",
      "Epoch [375/500], Train Loss: 0.0490, Val Loss: 0.0417\n",
      "Epoch [376/500], Train Loss: 0.0419, Val Loss: 0.0414\n",
      "Epoch [377/500], Train Loss: 0.0544, Val Loss: 0.0431\n",
      "Epoch [378/500], Train Loss: 0.0507, Val Loss: 0.0375\n",
      "Epoch [379/500], Train Loss: 0.0567, Val Loss: 0.0375\n",
      "Epoch [380/500], Train Loss: 0.0525, Val Loss: 0.0410\n",
      "Epoch [381/500], Train Loss: 0.0480, Val Loss: 0.0399\n",
      "Epoch [382/500], Train Loss: 0.0475, Val Loss: 0.0392\n",
      "Epoch [383/500], Train Loss: 0.0500, Val Loss: 0.0335\n",
      "Epoch [384/500], Train Loss: 0.0666, Val Loss: 0.0331\n",
      "Epoch [385/500], Train Loss: 0.0468, Val Loss: 0.0350\n",
      "Epoch [386/500], Train Loss: 0.0491, Val Loss: 0.0447\n",
      "Epoch [387/500], Train Loss: 0.0412, Val Loss: 0.0342\n",
      "Epoch [388/500], Train Loss: 0.0484, Val Loss: 0.0332\n",
      "Epoch [389/500], Train Loss: 0.0558, Val Loss: 0.0509\n",
      "Epoch [390/500], Train Loss: 0.0562, Val Loss: 0.0406\n",
      "Epoch [391/500], Train Loss: 0.0448, Val Loss: 0.0343\n",
      "Epoch [392/500], Train Loss: 0.0519, Val Loss: 0.0460\n",
      "Epoch [393/500], Train Loss: 0.0579, Val Loss: 0.0367\n",
      "Epoch [394/500], Train Loss: 0.0572, Val Loss: 0.0363\n",
      "Epoch [395/500], Train Loss: 0.0590, Val Loss: 0.0361\n",
      "Epoch [396/500], Train Loss: 0.0483, Val Loss: 0.0344\n",
      "Epoch [397/500], Train Loss: 0.0547, Val Loss: 0.0464\n",
      "Epoch [398/500], Train Loss: 0.0494, Val Loss: 0.0414\n",
      "Epoch [399/500], Train Loss: 0.0535, Val Loss: 0.0338\n",
      "Epoch [400/500], Train Loss: 0.0531, Val Loss: 0.0478\n",
      "Epoch [401/500], Train Loss: 0.0506, Val Loss: 0.0362\n",
      "Epoch [402/500], Train Loss: 0.0486, Val Loss: 0.0426\n",
      "Epoch [403/500], Train Loss: 0.0465, Val Loss: 0.0373\n",
      "Epoch [404/500], Train Loss: 0.0610, Val Loss: 0.0483\n",
      "Epoch [405/500], Train Loss: 0.0572, Val Loss: 0.0342\n",
      "Epoch [406/500], Train Loss: 0.0453, Val Loss: 0.0392\n",
      "Epoch [407/500], Train Loss: 0.0447, Val Loss: 0.0399\n",
      "Epoch [408/500], Train Loss: 0.0500, Val Loss: 0.0455\n",
      "Epoch [409/500], Train Loss: 0.0592, Val Loss: 0.0387\n",
      "Epoch [410/500], Train Loss: 0.0467, Val Loss: 0.0444\n",
      "Epoch [411/500], Train Loss: 0.0530, Val Loss: 0.0379\n",
      "Epoch [412/500], Train Loss: 0.0546, Val Loss: 0.0534\n",
      "Epoch [413/500], Train Loss: 0.0553, Val Loss: 0.0422\n",
      "Epoch [414/500], Train Loss: 0.0575, Val Loss: 0.0412\n",
      "Epoch [415/500], Train Loss: 0.0490, Val Loss: 0.0383\n",
      "Epoch [416/500], Train Loss: 0.0490, Val Loss: 0.0376\n",
      "Epoch [417/500], Train Loss: 0.0539, Val Loss: 0.0451\n",
      "Epoch [418/500], Train Loss: 0.0438, Val Loss: 0.0367\n",
      "Epoch [419/500], Train Loss: 0.0456, Val Loss: 0.0395\n",
      "Epoch [420/500], Train Loss: 0.0510, Val Loss: 0.0433\n",
      "Epoch [421/500], Train Loss: 0.0410, Val Loss: 0.0359\n",
      "Epoch [422/500], Train Loss: 0.0532, Val Loss: 0.0367\n",
      "Epoch [423/500], Train Loss: 0.0448, Val Loss: 0.0378\n",
      "Epoch [424/500], Train Loss: 0.0497, Val Loss: 0.0356\n",
      "Epoch [425/500], Train Loss: 0.0525, Val Loss: 0.0380\n",
      "Epoch [426/500], Train Loss: 0.0457, Val Loss: 0.0401\n",
      "Epoch [427/500], Train Loss: 0.0483, Val Loss: 0.0511\n",
      "Epoch [428/500], Train Loss: 0.0567, Val Loss: 0.0469\n",
      "Epoch [429/500], Train Loss: 0.0521, Val Loss: 0.0454\n",
      "Epoch [430/500], Train Loss: 0.0524, Val Loss: 0.0548\n",
      "Epoch [431/500], Train Loss: 0.0507, Val Loss: 0.0404\n",
      "Epoch [432/500], Train Loss: 0.0580, Val Loss: 0.0372\n",
      "Epoch [433/500], Train Loss: 0.0498, Val Loss: 0.0380\n",
      "Epoch [434/500], Train Loss: 0.0457, Val Loss: 0.0381\n",
      "Epoch [435/500], Train Loss: 0.0487, Val Loss: 0.0384\n",
      "Epoch [436/500], Train Loss: 0.0587, Val Loss: 0.0535\n",
      "Epoch [437/500], Train Loss: 0.0527, Val Loss: 0.0426\n",
      "Epoch [438/500], Train Loss: 0.0498, Val Loss: 0.0411\n",
      "Epoch [439/500], Train Loss: 0.0514, Val Loss: 0.0408\n",
      "Epoch [440/500], Train Loss: 0.0508, Val Loss: 0.0376\n",
      "Epoch [441/500], Train Loss: 0.0484, Val Loss: 0.0375\n",
      "Epoch [442/500], Train Loss: 0.0534, Val Loss: 0.0619\n",
      "Epoch [443/500], Train Loss: 0.0538, Val Loss: 0.0364\n",
      "Epoch [444/500], Train Loss: 0.0489, Val Loss: 0.0349\n",
      "Epoch [445/500], Train Loss: 0.0471, Val Loss: 0.0356\n",
      "Epoch [446/500], Train Loss: 0.0466, Val Loss: 0.0409\n",
      "Epoch [447/500], Train Loss: 0.0507, Val Loss: 0.0405\n",
      "Epoch [448/500], Train Loss: 0.0459, Val Loss: 0.0351\n",
      "Epoch [449/500], Train Loss: 0.0557, Val Loss: 0.0743\n",
      "Epoch [450/500], Train Loss: 0.0563, Val Loss: 0.0381\n",
      "Epoch [451/500], Train Loss: 0.0457, Val Loss: 0.0452\n",
      "Epoch [452/500], Train Loss: 0.0568, Val Loss: 0.0373\n",
      "Epoch [453/500], Train Loss: 0.0558, Val Loss: 0.0463\n",
      "Epoch [454/500], Train Loss: 0.0550, Val Loss: 0.0373\n",
      "Epoch [455/500], Train Loss: 0.0442, Val Loss: 0.0376\n",
      "Epoch [456/500], Train Loss: 0.0464, Val Loss: 0.0486\n",
      "Epoch [457/500], Train Loss: 0.0469, Val Loss: 0.0345\n",
      "Epoch [458/500], Train Loss: 0.0474, Val Loss: 0.0427\n",
      "Epoch [459/500], Train Loss: 0.0482, Val Loss: 0.0446\n",
      "Epoch [460/500], Train Loss: 0.0542, Val Loss: 0.0443\n",
      "Epoch [461/500], Train Loss: 0.0616, Val Loss: 0.0365\n",
      "Epoch [462/500], Train Loss: 0.0515, Val Loss: 0.0374\n",
      "Epoch [463/500], Train Loss: 0.0506, Val Loss: 0.0493\n",
      "Epoch [464/500], Train Loss: 0.0553, Val Loss: 0.0522\n",
      "Epoch [465/500], Train Loss: 0.0553, Val Loss: 0.0489\n",
      "Epoch [466/500], Train Loss: 0.0494, Val Loss: 0.0389\n",
      "Epoch [467/500], Train Loss: 0.0472, Val Loss: 0.0336\n",
      "Epoch [468/500], Train Loss: 0.0504, Val Loss: 0.0347\n",
      "Epoch [469/500], Train Loss: 0.0504, Val Loss: 0.0491\n",
      "Epoch [470/500], Train Loss: 0.0473, Val Loss: 0.0399\n",
      "Epoch [471/500], Train Loss: 0.0538, Val Loss: 0.0375\n",
      "Epoch [472/500], Train Loss: 0.0545, Val Loss: 0.0406\n",
      "Epoch [473/500], Train Loss: 0.0558, Val Loss: 0.0371\n",
      "Epoch [474/500], Train Loss: 0.0542, Val Loss: 0.0428\n",
      "Epoch [475/500], Train Loss: 0.0453, Val Loss: 0.0335\n",
      "Epoch [476/500], Train Loss: 0.0415, Val Loss: 0.0386\n",
      "Epoch [477/500], Train Loss: 0.0515, Val Loss: 0.0466\n",
      "Epoch [478/500], Train Loss: 0.0525, Val Loss: 0.0825\n",
      "Epoch [479/500], Train Loss: 0.0629, Val Loss: 0.0422\n",
      "Epoch [480/500], Train Loss: 0.0556, Val Loss: 0.0386\n",
      "Epoch [481/500], Train Loss: 0.0531, Val Loss: 0.0422\n",
      "Epoch [482/500], Train Loss: 0.0599, Val Loss: 0.0396\n",
      "Epoch [483/500], Train Loss: 0.0519, Val Loss: 0.0376\n",
      "Epoch [484/500], Train Loss: 0.0531, Val Loss: 0.0397\n",
      "Epoch [485/500], Train Loss: 0.0471, Val Loss: 0.0349\n",
      "Epoch [486/500], Train Loss: 0.0459, Val Loss: 0.0375\n",
      "Epoch [487/500], Train Loss: 0.0444, Val Loss: 0.0582\n",
      "Epoch [488/500], Train Loss: 0.0477, Val Loss: 0.0483\n",
      "Epoch [489/500], Train Loss: 0.0496, Val Loss: 0.0376\n",
      "Epoch [490/500], Train Loss: 0.0501, Val Loss: 0.0365\n",
      "Epoch [491/500], Train Loss: 0.0446, Val Loss: 0.0464\n",
      "Epoch [492/500], Train Loss: 0.0457, Val Loss: 0.0385\n",
      "Epoch [493/500], Train Loss: 0.0492, Val Loss: 0.0342\n",
      "Epoch [494/500], Train Loss: 0.0391, Val Loss: 0.0399\n",
      "Epoch [495/500], Train Loss: 0.0508, Val Loss: 0.0399\n",
      "Epoch [496/500], Train Loss: 0.0523, Val Loss: 0.0364\n",
      "Epoch [497/500], Train Loss: 0.0521, Val Loss: 0.0380\n",
      "Epoch [498/500], Train Loss: 0.0522, Val Loss: 0.0358\n",
      "Epoch [499/500], Train Loss: 0.0482, Val Loss: 0.0378\n",
      "Epoch [500/500], Train Loss: 0.0499, Val Loss: 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/_44jn7jx03b17y35qvlp17sr0000gn/T/ipykernel_75151/1260078627.py:58: UserWarning: Using a target size (torch.Size([1, 0, 5])) that is different to the input size (torch.Size([1, 5, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss_x = F.mse_loss(x_n_recon, x_n, reduction='mean') / (H * W)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 158\u001b[0m\n\u001b[1;32m    155\u001b[0m         x_n_recon, r_n_recon, mu, log_var \u001b[38;5;241m=\u001b[39m model(x, y, x_c, y_c)\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_n_recon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_n\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_n_recon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(x_n, x_n_recon, r_n, r_n_recon, mu, log_var)\u001b[0m\n\u001b[1;32m     55\u001b[0m r_n \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(r_n, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r_n\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m r_n\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Calculate the mean squared error, normalized by the number of elements (H*W)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m recon_loss_x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_n_recon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (H \u001b[38;5;241m*\u001b[39m W)\n\u001b[1;32m     59\u001b[0m recon_loss_r \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(r_n_recon, r_n, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate the Kullback-Leibler divergence\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 25\n",
    "hidden_size = 100\n",
    "latent_size = 5\n",
    "context_size = 5\n",
    "input_size = 1\n",
    "output_size = 5\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "alpha = 1.0\n",
    "beta = 1e-5\n",
    "\n",
    "\n",
    "# Reshape the input data to the desired 5-dimensional shape\n",
    "batch_size_train = X_train.shape[0]\n",
    "batch_size_val = X_val.shape[0]\n",
    "batch_size_test = X_test.shape[0]\n",
    "sequence_length = 1  # \n",
    "num_surfaces = 1  # the number of surfaces to generate\n",
    "\n",
    "\n",
    "H = 5  # Height of the IV surface grid\n",
    "W = 5  # Width of the IV surface grid\n",
    "\n",
    "#print(X_train.shape)\n",
    "X_train_reshaped = X_train.reshape(batch_size_train,  H, W)\n",
    "X_val_reshaped = X_val.reshape(batch_size_val, H, W)\n",
    "X_test_reshaped = X_test.reshape(batch_size_test, H, W)\n",
    "#print(X_train_reshaped.shape)\n",
    "extra_features_size = 3\n",
    "\n",
    "y_train_reshaped = y_train.reshape(y_train.shape[0], 1, extra_features_size)\n",
    "y_val_reshaped = y_val.reshape(y_val.shape[0], 1, extra_features_size)\n",
    "y_test_reshaped = y_test.reshape(y_test.shape[0], 1, extra_features_size)\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train_reshaped), torch.Tensor(y_train_reshaped))\n",
    "val_data = TensorDataset(torch.Tensor(X_val_reshaped), torch.Tensor(y_val_reshaped))\n",
    "test_data = TensorDataset(torch.Tensor(X_test_reshaped), torch.Tensor(y_test_reshaped))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the model\n",
    "model = CVAE(input_size, hidden_size, latent_size, context_size, input_size, num_surfaces)\n",
    "\n",
    "def loss_function(x_n, x_n_recon, r_n, r_n_recon, mu, log_var):\n",
    "    # Ensure x_n and r_n are unsqueezed if needed (depends on data shape handling in other parts of your code)\n",
    "    x_n = torch.unsqueeze(x_n, dim=0) if len(x_n.shape) < 3 else x_n\n",
    "    r_n = torch.unsqueeze(r_n, dim=0) if len(r_n.shape) < 3 else r_n\n",
    "\n",
    "    # Calculate the mean squared error, normalized by the number of elements (H*W)\n",
    "    recon_loss_x = F.mse_loss(x_n_recon, x_n, reduction='mean') / (H * W)\n",
    "    recon_loss_r = F.mse_loss(r_n_recon, r_n, reduction='mean')\n",
    "\n",
    "    # Calculate the Kullback-Leibler divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Combine the losses with the scaling factors\n",
    "    total_loss = recon_loss_x + alpha * recon_loss_r + beta * kl_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        batch_size = x.shape[0] #batch_size, context_length, num_surfaces, H, W = x.shape\n",
    "        \n",
    "        # Generate a random day t to split the sequence into context and future\n",
    "        t = batch_size - num_surfaces - 1\n",
    "        #print(t)\n",
    "\n",
    "        x_c, x_n = x[:t, :], x[t:, :]\n",
    "        y_c, y_n = y[:t, :], y[t:, :]\n",
    "            \n",
    "        r_n = y_n[:, :, 0]\n",
    "        \n",
    "        x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            batch_size = x.shape[0]  # Number of days in the sequence\n",
    "            \n",
    "            # Generate a random day t to split the sequence into context and future\n",
    "            #t = generate_random_day(batch_size)\n",
    "            t = batch_size - num_surfaces - 1\n",
    "            \n",
    "            x_c, x_n = x[:t, :], x[t:, :]\n",
    "            y_c, y_n = y[:t, :], y[t:, :]\n",
    "            \n",
    "            # Extract the log return feature from y_n\n",
    "            r_n = y_n[:, :, 0]\n",
    "            \n",
    "            x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "            #print(f\"x_n_recon: {x_n_recon}\")\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Testing\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        batch_size = x.shape[0]  # Number of days in the sequence\n",
    "        \n",
    "        # Generate a random day t to split the sequence into context and future\n",
    "        t = batch_size - num_surfaces - 1\n",
    "        \n",
    "        x_c, x_n = x[:, :t], x[:, t:]\n",
    "        y_c, y_n = y[:, :t], y[:, t:]\n",
    "        \n",
    "        # Extract the log return feature from y_n\n",
    "        r_n = y_n[:, :, 0]\n",
    "        \n",
    "        x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader):.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a819b88-e72d-4605-9e71-94db5afb8752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
