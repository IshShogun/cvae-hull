{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96622b7-9564-4921-a263-ba72644bbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CNN and MLP primitive\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##The encoder in the paper takes x in R^(TxHxW) and y in R^(TxE) and maps them to R^(TxL) the dimensionality of the context encoder\n",
    "##and encoder is set to 5. So this cnn needs to output (32x5) so each (surface in R^(5x5) -> (z in R^5)\n",
    "\n",
    "##we enhance the dimensionality of the iv surface first by upgrading the number of channels. The reasoning behind this is similar to why \n",
    "##we do this in transformer architecture. A larger dimensional space will be able to capture more nuanced information and represent it in number form\n",
    "##then we compress this to something digestable\n",
    "class CNN(nn.Module):\n",
    "    #input_size and output_size represent the number of channels in the input and output data\n",
    "    #channels is the number of dimensions a single data point will have ie RGB = 3 channelss\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(output_size * 5 * 5, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, H, W = x.shape\n",
    "        x = x.reshape(batch_size, 1, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(f\"x shape before resize and convultion passthrough is {x.shape}\")\n",
    "        #x = x.reshape(batch_size, 1, 5)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        #print(f\"after passthrough into convultion layers and fully connected layer ther shape of x is {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = F.relu(self.fc1(y))\n",
    "        x = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377a416f-980c-4024-bb50-115fb221343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CNN For decoder \n",
    "\n",
    "#due to difference in dimensionality - What happened st i needed to make this. z, zeta make different shape so needed redfinition\n",
    "##plus it will define a new coniditional probability distribtution\n",
    "\n",
    "##had to make another cnn for decoder due difference in dimensionality\n",
    "\n",
    "##in here the output size should be the number of days in the future?\n",
    "\n",
    "##for now output_size = 1 so we predict 1 day into the future?\n",
    "class TCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_surfaces):\n",
    "        super(TCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5 * 5 * num_surfaces)\n",
    "        self.output_size = output_size\n",
    "        self.num_surfaces = num_surfaces\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, self.num_surfaces, 5, 5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3016d2ec-e952-4aa8-ad75-2cf4b43b5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ENCODER DECODER CONTEXTENCODER\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cnn = CNN(input_size, 5)\n",
    "        self.mlp = nn.Identity()\n",
    "        self.lstm = nn.LSTM(5 + 3, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.linear_sigma = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_encoded = self.cnn(x)\n",
    "        y_encoded = self.mlp(y)\n",
    "        y_encoded = torch.squeeze(y_encoded, dim=1)\n",
    "        #print(f\"x_encoded shape is {x_encoded.shape}  y_encoded shape is {y_encoded.shape}\")\n",
    "        encoded = torch.cat((x_encoded, y_encoded), dim=-1)\n",
    "        #print(f\"concatenated vector is of size {encoded.shape}\")\n",
    "        _, (hidden, _) = self.lstm(encoded)\n",
    "        #print(\"hidden state created\")\n",
    "        hidden = hidden[-1]  # Take the last hidden state\n",
    "        mu = self.linear_mu(hidden)\n",
    "        log_var = self.linear_sigma(hidden)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        #print(\"encoding successful\")\n",
    "        return z, mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, context_size):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.cnn = CNN(input_size, 5)\n",
    "        self.mlp = nn.Identity()\n",
    "        self.lstm = nn.LSTM(5 + 3, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, context_size)\n",
    "\n",
    "    def forward(self, x_c, y_c):\n",
    "        x_encoded = self.cnn(x_c)\n",
    "        y_encoded = self.mlp(y_c)\n",
    "        y_encoded = torch.squeeze(y_encoded, dim=1)\n",
    "        #print(f\"x_encoded size = {x_encoded.shape} ||y_encoded size = {y_encoded.shape}\")\n",
    "        encoded = torch.cat((x_encoded, y_encoded), dim=-1)\n",
    "        _, (hidden, _) = self.lstm(encoded)\n",
    "        hidden = hidden[-1]  # Take the last hidden state\n",
    "        zeta = self.linear(hidden)\n",
    "        #print(\"context encoding successful\")\n",
    "        return zeta\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_surfaces):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_size + 5, hidden_size, num_layers=2, batch_first=True, dropout=0.2) #latent\n",
    "        self.tcnn = TCNN(hidden_size, output_size, num_surfaces)\n",
    "        self.mlp = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, z, zeta):\n",
    "        # Reshape z and zeta to have shape (1, 5)\n",
    "        z = z.view(1, -1)\n",
    "        zeta = zeta.view(1, -1)\n",
    "        # Concatenate z and zeta along the second dimension to get shape (1, 10)\n",
    "        z_concat = torch.cat((z, zeta), dim=1)\n",
    "        \n",
    "        #print(f\"z_concat is of size {z_concat.shape}\")\n",
    "        hidden, _ = self.lstm(z_concat)\n",
    "        #print('i am here')\n",
    "        #print(f\"hidden state published, hidden state shape {hidden.shape}\")\n",
    "        x_n = self.tcnn(hidden) \n",
    "        #print('i am here')\n",
    "        r_n = self.mlp(hidden)\n",
    "        #print(\"decoding successful\")\n",
    "        return torch.squeeze(x_n, dim=0), torch.squeeze(r_n, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e680583a-05fe-4c3b-bfa2-57a888c03fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moneyness levels and time to maturity (in days ##based of paper single skew and slope for surface\n",
    "moneyness_levels = [0.7, 0.85, 1, 1.15, 1.3]\n",
    "ttm_levels = [1, 3, 6, 12, 24] #days\n",
    "\n",
    "def calculate_skew_slope(iv_row):\n",
    "    # Select IV values for the specific TTM and moneyness levels\n",
    "    iv_12d_085 = iv_row.get((12, 0.85), 0)  # IV for 1 year TTM and moneyness=0.85\n",
    "    iv_12d_100 = iv_row.get((12, 1.00), 0)  # IV for 1 year TTM and moneyness=1.00\n",
    "    iv_12d_115 = iv_row.get((12, 1.15), 0)  # IV for 1 year TTM and moneyness=1.15\n",
    "\n",
    "    # Calculate skew\n",
    "    skew = (iv_12d_085 + iv_12d_115) / 2 - iv_12d_100 if iv_12d_100 else 0  # Avoid division by zero\n",
    "\n",
    "    # Select IV values for slope calculation\n",
    "    iv_3d_100 = iv_row.get((3, 1.00), 0)  # IV for 3 months TTM and moneyness=1.00\n",
    "    iv_24d_100 = iv_row.get((24, 1.00), 0)  # IV for 2 years TTM and moneyness=1.00\n",
    "\n",
    "    # Calculate slope\n",
    "    slope = iv_24d_100 - iv_3d_100\n",
    "\n",
    "    return skew, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16fe4a5a-20e5-4168-bd9c-03ab231b460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CVAE\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, context_size, output_size, num_surfaces):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, latent_size)\n",
    "        self.context_encoder = ContextEncoder(input_size, hidden_size, context_size)\n",
    "        self.decoder = Decoder(latent_size, hidden_size, output_size, num_surfaces)\n",
    "        self.latent_size = latent_size\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def forward(self, x, y, x_c, y_c):\n",
    "        #print('1')\n",
    "        z, mu, log_var = self.encoder(x, y) ##we have sampled z from distribution here\n",
    "        #print(f\"The shape of latent representation z is {z.shape}\")\n",
    "        #print('2')\n",
    "        zeta = self.context_encoder(x_c, y_c) ##we have sampled zeta from distribution here\n",
    "        #print(f\"the shape of zeta (encoded context) is {zeta.shape}\")\n",
    "        x_n, r_n = self.decoder(z, zeta)\n",
    "        return x_n, r_n, mu, log_var\n",
    "\n",
    "    \n",
    "    def generate(self, x_c, y_c, ttm):\n",
    "        x_hat = []\n",
    "        r_hat = []\n",
    "    \n",
    "        for i in range(0, ttm):\n",
    "            # reasing z and context\n",
    "            z = torch.tensor(np.array([np.random.normal(loc=0, scale=1, size=5)]), dtype=torch.float32)\n",
    "            zeta = self.context_encoder(x_c, y_c)\n",
    "            x_n, r_n = self.decoder(z, zeta)\n",
    "            x_hat.append(x_n)\n",
    "            r_hat.append(r_n)\n",
    "    \n",
    "            # Update x_c by removing the first value and appending x_n\n",
    "            x_c = torch.cat((x_c[1:], x_n[0].unsqueeze(0)))\n",
    "    \n",
    "            # Convert x_n (generated IV surfaces) to a dictionary format\n",
    "            iv_row = {(ttm, moneyness): x_n[0][i, j] for i, moneyness in enumerate(moneyness_levels) for j, ttm in enumerate(ttm_levels)}\n",
    "    \n",
    "            # Calculate skew and slope using the calculate_skew_slope function\n",
    "            skew, slope = calculate_skew_slope(iv_row)\n",
    "           # print(skew)\n",
    "            #print(slope)\n",
    "    \n",
    "            # Create y_n by combining r_n (predicted log returns), skew, and slope\n",
    "            y_n = torch.cat((r_n, torch.tensor([skew, slope])))\n",
    "\n",
    "            y_n = y_n.view(1, -1)\n",
    "            #print(y_c)\n",
    "            #print(y_n)\n",
    "            # Update y_c by removing the first value and appending y_n\n",
    "            y_c = torch.cat((y_c[1:], y_n.unsqueeze(0)))\n",
    "    \n",
    "        return x_hat, r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3be940-4695-424d-8c99-f1964625f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4182, 5, 5)\n",
      "X_val shape: (896, 5, 5)\n",
      "X_test shape: (897, 5, 5)\n",
      "y_train shape: (4182, 3)\n",
      "y_val shape: (896, 3)\n",
      "y_test shape: (897, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the DataFrame from the CSV file\n",
    "df = pd.read_csv('cvae_training_data.csv')\n",
    "\n",
    "# Extract IV surfaces and reshape them into 5x5 matrices\n",
    "X = np.array([np.fromstring(iv_surface[1:-1], sep=' ').reshape(5, 5) for iv_surface in df['iv_surface_flattened']])\n",
    "Y = df[['skew', 'slope', 'log_returns']].values\n",
    "\n",
    "# Normalize features\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Flatten the IV surfaces for scaling, then reshape back to (5, 5)\n",
    "X_flat = np.array([surface.flatten() for surface in X])\n",
    "X_scaled = scaler_x.fit_transform(X_flat).reshape(-1, 5, 5)\n",
    "Y_scaled = scaler_y.fit_transform(Y)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, Y_scaled, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a68456d-e530-49b0-847a-d1ea1f5f2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_day(n):\n",
    "    \"\"\"\n",
    "    Generates a random number between 1 and n\n",
    "    \"\"\"\n",
    "    # Generate a random number between 1 and n\n",
    "    random_number = random.randint(1, n - 1)\n",
    "    return random_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "503837b1-b9d2-4aba-b37f-3a91318a0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "###PREPARE DATA\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils import mkldnn as mkldnn_utils\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 100\n",
    "latent_size = 5\n",
    "context_size = 5\n",
    "input_size = 1\n",
    "output_size = 5\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "alpha = 0.1\n",
    "beta = 1e-5\n",
    "\n",
    "\n",
    "# Reshape the input data to the desired 5-dimensional shape\n",
    "batch_size_train = X_train.shape[0]\n",
    "batch_size_val = X_val.shape[0]\n",
    "batch_size_test = X_test.shape[0]\n",
    "sequence_length = 1  # \n",
    "num_surfaces = 1  # the number of surfaces to generate\n",
    "\n",
    "\n",
    "H = 5  # Height of the IV surface grid\n",
    "W = 5  # Width of the IV surface grid\n",
    "\n",
    "#print(X_train.shape)\n",
    "X_train_reshaped = X_train.reshape(batch_size_train,  H, W)\n",
    "X_val_reshaped = X_val.reshape(batch_size_val, H, W)\n",
    "X_test_reshaped = X_test.reshape(batch_size_test, H, W)\n",
    "#print(X_train_reshaped.shape)\n",
    "extra_features_size = 3\n",
    "\n",
    "y_train_reshaped = y_train.reshape(y_train.shape[0], 1, extra_features_size)\n",
    "y_val_reshaped = y_val.reshape(y_val.shape[0], 1, extra_features_size)\n",
    "y_test_reshaped = y_test.reshape(y_test.shape[0], 1, extra_features_size)\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train_reshaped), torch.Tensor(y_train_reshaped))\n",
    "val_data = TensorDataset(torch.Tensor(X_val_reshaped), torch.Tensor(y_val_reshaped))\n",
    "test_data = TensorDataset(torch.Tensor(X_test_reshaped), torch.Tensor(y_test_reshaped))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the model\n",
    "model = CVAE(input_size, hidden_size, latent_size, context_size, output_size, num_surfaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73cfc4fe-13dd-4d85-b490-ab4eb4620f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/_44jn7jx03b17y35qvlp17sr0000gn/T/ipykernel_95153/3067755013.py:11: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss_r = F.mse_loss(r_n_recon, r_n, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.1644, Val Loss: 0.1917\n",
      "Epoch [2/100], Train Loss: 0.1502, Val Loss: 0.1636\n",
      "Epoch [3/100], Train Loss: 0.1033, Val Loss: 0.1065\n",
      "Epoch [4/100], Train Loss: 0.0838, Val Loss: 0.0406\n",
      "Epoch [5/100], Train Loss: 0.0439, Val Loss: 0.0329\n",
      "Epoch [6/100], Train Loss: 0.0347, Val Loss: 0.0247\n",
      "Epoch [7/100], Train Loss: 0.0378, Val Loss: 0.0295\n",
      "Epoch [8/100], Train Loss: 0.0263, Val Loss: 0.0294\n",
      "Epoch [9/100], Train Loss: 0.0248, Val Loss: 0.0265\n",
      "Epoch [10/100], Train Loss: 0.0297, Val Loss: 0.0309\n",
      "Epoch [11/100], Train Loss: 0.0265, Val Loss: 0.0235\n",
      "Epoch [12/100], Train Loss: 0.0296, Val Loss: 0.0261\n",
      "Epoch [13/100], Train Loss: 0.0217, Val Loss: 0.0229\n",
      "Epoch [14/100], Train Loss: 0.0297, Val Loss: 0.0253\n",
      "Epoch [15/100], Train Loss: 0.0231, Val Loss: 0.0233\n",
      "Epoch [16/100], Train Loss: 0.0265, Val Loss: 0.0303\n",
      "Epoch [17/100], Train Loss: 0.0148, Val Loss: 0.0241\n",
      "Epoch [18/100], Train Loss: 0.0235, Val Loss: 0.0224\n",
      "Epoch [19/100], Train Loss: 0.0212, Val Loss: 0.0238\n",
      "Epoch [20/100], Train Loss: 0.0207, Val Loss: 0.0292\n",
      "Epoch [21/100], Train Loss: 0.0246, Val Loss: 0.0236\n",
      "Epoch [22/100], Train Loss: 0.0189, Val Loss: 0.0234\n",
      "Epoch [23/100], Train Loss: 0.0110, Val Loss: 0.0296\n",
      "Epoch [24/100], Train Loss: 0.0155, Val Loss: 0.0279\n",
      "Epoch [25/100], Train Loss: 0.0166, Val Loss: 0.0268\n",
      "Epoch [26/100], Train Loss: 0.0160, Val Loss: 0.0213\n",
      "Epoch [27/100], Train Loss: 0.0143, Val Loss: 0.0214\n",
      "Epoch [28/100], Train Loss: 0.0124, Val Loss: 0.0190\n",
      "Epoch [29/100], Train Loss: 0.0126, Val Loss: 0.0250\n",
      "Epoch [30/100], Train Loss: 0.0217, Val Loss: 0.0238\n",
      "Epoch [31/100], Train Loss: 0.0170, Val Loss: 0.0207\n",
      "Epoch [32/100], Train Loss: 0.0153, Val Loss: 0.0251\n",
      "Epoch [33/100], Train Loss: 0.0162, Val Loss: 0.0309\n",
      "Epoch [34/100], Train Loss: 0.0123, Val Loss: 0.0226\n",
      "Epoch [35/100], Train Loss: 0.0215, Val Loss: 0.0254\n",
      "Epoch [36/100], Train Loss: 0.0203, Val Loss: 0.0262\n",
      "Epoch [37/100], Train Loss: 0.0169, Val Loss: 0.0215\n",
      "Epoch [38/100], Train Loss: 0.0162, Val Loss: 0.0207\n",
      "Epoch [39/100], Train Loss: 0.0102, Val Loss: 0.0277\n",
      "Epoch [40/100], Train Loss: 0.0202, Val Loss: 0.0191\n",
      "Epoch [41/100], Train Loss: 0.0200, Val Loss: 0.0205\n",
      "Epoch [42/100], Train Loss: 0.0158, Val Loss: 0.0216\n",
      "Epoch [43/100], Train Loss: 0.0107, Val Loss: 0.0205\n",
      "Epoch [44/100], Train Loss: 0.0105, Val Loss: 0.0209\n",
      "Epoch [45/100], Train Loss: 0.0178, Val Loss: 0.0207\n",
      "Epoch [46/100], Train Loss: 0.0118, Val Loss: 0.0213\n",
      "Epoch [47/100], Train Loss: 0.0117, Val Loss: 0.0231\n",
      "Epoch [48/100], Train Loss: 0.0137, Val Loss: 0.0194\n",
      "Epoch [49/100], Train Loss: 0.0121, Val Loss: 0.0195\n",
      "Epoch [50/100], Train Loss: 0.0133, Val Loss: 0.0243\n",
      "Epoch [51/100], Train Loss: 0.0143, Val Loss: 0.0198\n",
      "Epoch [52/100], Train Loss: 0.0109, Val Loss: 0.0182\n",
      "Epoch [53/100], Train Loss: 0.0205, Val Loss: 0.0186\n",
      "Epoch [54/100], Train Loss: 0.0133, Val Loss: 0.0212\n",
      "Epoch [55/100], Train Loss: 0.0130, Val Loss: 0.0198\n",
      "Epoch [56/100], Train Loss: 0.0174, Val Loss: 0.0183\n",
      "Epoch [57/100], Train Loss: 0.0128, Val Loss: 0.0193\n",
      "Epoch [58/100], Train Loss: 0.0138, Val Loss: 0.0225\n",
      "Epoch [59/100], Train Loss: 0.0201, Val Loss: 0.0192\n",
      "Epoch [60/100], Train Loss: 0.0128, Val Loss: 0.0255\n",
      "Epoch [61/100], Train Loss: 0.0123, Val Loss: 0.0200\n",
      "Epoch [62/100], Train Loss: 0.0125, Val Loss: 0.0184\n",
      "Epoch [63/100], Train Loss: 0.0182, Val Loss: 0.0181\n",
      "Epoch [64/100], Train Loss: 0.0183, Val Loss: 0.0237\n",
      "Epoch [65/100], Train Loss: 0.0110, Val Loss: 0.0197\n",
      "Epoch [66/100], Train Loss: 0.0093, Val Loss: 0.0180\n",
      "Epoch [67/100], Train Loss: 0.0135, Val Loss: 0.0198\n",
      "Epoch [68/100], Train Loss: 0.0083, Val Loss: 0.0194\n",
      "Epoch [69/100], Train Loss: 0.0143, Val Loss: 0.0214\n",
      "Epoch [70/100], Train Loss: 0.0152, Val Loss: 0.0170\n",
      "Epoch [71/100], Train Loss: 0.0143, Val Loss: 0.0162\n",
      "Epoch [72/100], Train Loss: 0.0127, Val Loss: 0.0182\n",
      "Epoch [73/100], Train Loss: 0.0103, Val Loss: 0.0177\n",
      "Epoch [74/100], Train Loss: 0.0113, Val Loss: 0.0181\n",
      "Epoch [75/100], Train Loss: 0.0108, Val Loss: 0.0192\n",
      "Epoch [76/100], Train Loss: 0.0142, Val Loss: 0.0187\n",
      "Epoch [77/100], Train Loss: 0.0207, Val Loss: 0.0176\n",
      "Epoch [78/100], Train Loss: 0.0144, Val Loss: 0.0187\n",
      "Epoch [79/100], Train Loss: 0.0118, Val Loss: 0.0159\n",
      "Epoch [80/100], Train Loss: 0.0113, Val Loss: 0.0187\n",
      "Epoch [81/100], Train Loss: 0.0116, Val Loss: 0.0201\n",
      "Epoch [82/100], Train Loss: 0.0169, Val Loss: 0.0228\n",
      "Epoch [83/100], Train Loss: 0.0131, Val Loss: 0.0196\n",
      "Epoch [84/100], Train Loss: 0.0213, Val Loss: 0.0183\n",
      "Epoch [85/100], Train Loss: 0.0119, Val Loss: 0.0180\n",
      "Epoch [86/100], Train Loss: 0.0111, Val Loss: 0.0168\n",
      "Epoch [87/100], Train Loss: 0.0085, Val Loss: 0.0200\n",
      "Epoch [88/100], Train Loss: 0.0124, Val Loss: 0.0202\n",
      "Epoch [89/100], Train Loss: 0.0098, Val Loss: 0.0223\n",
      "Epoch [90/100], Train Loss: 0.0156, Val Loss: 0.0185\n",
      "Epoch [91/100], Train Loss: 0.0127, Val Loss: 0.0181\n",
      "Epoch [92/100], Train Loss: 0.0122, Val Loss: 0.0125\n",
      "Epoch [93/100], Train Loss: 0.0097, Val Loss: 0.0206\n",
      "Epoch [94/100], Train Loss: 0.0122, Val Loss: 0.0189\n",
      "Epoch [95/100], Train Loss: 0.0104, Val Loss: 0.0194\n",
      "Epoch [96/100], Train Loss: 0.0124, Val Loss: 0.0179\n",
      "Epoch [97/100], Train Loss: 0.0219, Val Loss: 0.0217\n",
      "Epoch [98/100], Train Loss: 0.0167, Val Loss: 0.0165\n",
      "Epoch [99/100], Train Loss: 0.0117, Val Loss: 0.0154\n",
      "Epoch [100/100], Train Loss: 0.0115, Val Loss: 0.0186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CVAE(\n",
       "  (encoder): Encoder(\n",
       "    (cnn): CNN(\n",
       "      (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fc): Linear(in_features=125, out_features=5, bias=True)\n",
       "    )\n",
       "    (mlp): Identity()\n",
       "    (lstm): LSTM(8, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    (linear_mu): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (linear_sigma): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       "  (context_encoder): ContextEncoder(\n",
       "    (cnn): CNN(\n",
       "      (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fc): Linear(in_features=125, out_features=5, bias=True)\n",
       "    )\n",
       "    (mlp): Identity()\n",
       "    (lstm): LSTM(8, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    (linear): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm): LSTM(10, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    (tcnn): TCNN(\n",
       "      (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (fc3): Linear(in_features=64, out_features=25, bias=True)\n",
       "    )\n",
       "    (mlp): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###TRAIN MODEL\n",
    "\n",
    "num_epochs = 500\n",
    "def loss_function(x_n, x_n_recon, r_n, r_n_recon, mu, log_var):\n",
    "    # Ensure x_n and r_n are unsqueezed if needed (depends on data shape handling in other parts of your code)\n",
    "    x_n = torch.unsqueeze(x_n, dim=0) if len(x_n.shape) < 3 else x_n\n",
    "    r_n = torch.unsqueeze(r_n, dim=0) if len(r_n.shape) < 3 else r_n\n",
    "\n",
    "    # Calculate the mean squared error, normalized by the number of elements (H*W)\n",
    "    recon_loss_x = F.mse_loss(x_n_recon, x_n, reduction='mean') / (H * W)\n",
    "    recon_loss_r = F.mse_loss(r_n_recon, r_n, reduction='mean')\n",
    "\n",
    "    # Calculate the Kullback-Leibler divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Combine the losses with the scaling factors\n",
    "    total_loss = recon_loss_x + alpha * recon_loss_r + beta * kl_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        batch_size = x.shape[0] #batch_size, context_length, num_surfaces, H, W = x.shape\n",
    "        \n",
    "        # Generate a random day t to split the sequence into context and future\n",
    "        t = batch_size - num_surfaces - 1\n",
    "        #print(t)\n",
    "\n",
    "        x_c, x_n = x[:t, :], x[t:, :]\n",
    "        y_c, y_n = y[:t, :], y[t:, :]\n",
    "            \n",
    "        r_n = y_n[:, :, 0]\n",
    "        \n",
    "        x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            batch_size = x.shape[0]  # Number of days in the sequence\n",
    "            \n",
    "            # Generate a random day t to split the sequence into context and future\n",
    "            #t = generate_random_day(batch_size)\n",
    "            t = batch_size - num_surfaces - 1\n",
    "            \n",
    "            x_c, x_n = x[:t, :], x[t:, :]\n",
    "            y_c, y_n = y[:t, :], y[t:, :]\n",
    "            \n",
    "            # Extract the log return feature from y_n\n",
    "            r_n = y_n[:, :, 0]\n",
    "            \n",
    "            x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "            #print(f\"x_n_recon: {x_n_recon}\")\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Testing\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c955b-9238-4128-9dc3-2274ab6dadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterSampler, KFold\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 25\n",
    "context_size = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "# Reshape the input data to the desired 5-dimensional shape\n",
    "batch_size_train = X_train.shape[0]\n",
    "batch_size_test = X_test.shape[0]\n",
    "\n",
    "sequence_length = 1\n",
    "num_surfaces = 1  # the number of surfaces to generate\n",
    "H = 5  # Height of the IV surface grid\n",
    "W = 5  # Width of the IV surface grid\n",
    "\n",
    "X_train_reshaped = X_train.reshape(batch_size_train, H, W)\n",
    "X_test_reshaped = X_test.reshape(batch_size_test, H, W)\n",
    "\n",
    "extra_features_size = 3\n",
    "y_train_reshaped = y_train.reshape(y_train.shape[0], 1, extra_features_size)\n",
    "y_test_reshaped = y_test.reshape(y_test.shape[0], 1, extra_features_size)\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train_reshaped), torch.Tensor(y_train_reshaped))\n",
    "test_data = TensorDataset(torch.Tensor(X_test_reshaped), torch.Tensor(y_test_reshaped))\n",
    "\n",
    "# Create the model\n",
    "model = CVAE(input_size, hidden_size, latent_size, context_size, output_size, num_surfaces)\n",
    "\n",
    "def loss_function(x_n, x_n_recon, r_n, r_n_recon, mu, log_var):\n",
    "    # Ensure x_n and r_n are unsqueezed if needed (depends on data shape handling in other parts of your code)\n",
    "    x_n = torch.unsqueeze(x_n, dim=0) if len(x_n.shape) < 3 else x_n\n",
    "    r_n = torch.unsqueeze(r_n, dim=0) if len(r_n.shape) < 3 else r_n\n",
    "\n",
    "    # Calculate the mean squared error, normalized by the number of elements (H*W)\n",
    "    recon_loss_x = F.mse_loss(x_n_recon, x_n, reduction='mean') / (H * W)\n",
    "    recon_loss_r = F.mse_loss(r_n_recon, r_n, reduction='mean')\n",
    "\n",
    "    # Calculate the Kullback-Leibler divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Combine the losses with the scaling factors\n",
    "    total_loss = recon_loss_x + alpha * recon_loss_r + beta * kl_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "param_distributions = {\n",
    "    #'hidden_size': [50, 100, 200, 300],\n",
    "   # 'latent_size': [5, 10, 20],\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "    'alpha': [0.1, 0.5, 1.0],\n",
    "    'beta': [1e-5, 5e-6, 1e-6],\n",
    "    #'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "num_epochs = 100\n",
    "num_iterations = 5\n",
    "random_search = ParameterSampler(param_distributions, n_iter=num_iterations, random_state=42)\n",
    "\n",
    "# Initialize variables to store the best hyperparameters and validation loss\n",
    "best_hyperparameters = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 2\n",
    "\n",
    "# Create a KFold object for cross-validation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for params in random_search:\n",
    "    #hidden_size = params['hidden_size']\n",
    "    #latent_size = params['latent_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    alpha = params['alpha']\n",
    "    beta = params['beta']\n",
    "    #batch_size = params['batch_size']\n",
    "    print(f\"Hyperparameters: {params}\")\n",
    "\n",
    "    # Initialize variables to store the total validation loss across all folds\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(X_train_reshaped)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # Create data subsets for the current fold\n",
    "        train_subset = TensorDataset(torch.Tensor(X_train_reshaped[train_indices]), torch.Tensor(y_train_reshaped[train_indices]))\n",
    "        val_subset = TensorDataset(torch.Tensor(X_train_reshaped[val_indices]), torch.Tensor(y_train_reshaped[val_indices]))\n",
    "\n",
    "        # Create data loaders for the current fold\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Create the model with the current hyperparameters\n",
    "        model = CVAE(input_size, hidden_size, latent_size, context_size, input_size, num_surfaces)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                x, y = batch\n",
    "                batch_size = x.shape[0]  # batch_size, context_length, num_surfaces, H, W = x.shape\n",
    "\n",
    "                # Generate a random day t to split the sequence into context and future\n",
    "                t = batch_size - num_surfaces - 1\n",
    "\n",
    "                x_c, x_n = x[:t, :], x[t:, :]\n",
    "                y_c, y_n = y[:t, :], y[t:, :]\n",
    "                r_n = y_n[:, :, 0]\n",
    "\n",
    "                x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x, y = batch\n",
    "                    batch_size = x.shape[0]  # Number of days in the sequence\n",
    "\n",
    "                    # Generate a random day t to split the sequence into context and future\n",
    "                    t = batch_size - num_surfaces - 1\n",
    "\n",
    "                    x_c, x_n = x[:t, :], x[t:, :]\n",
    "                    y_c, y_n = y[:t, :], y[t:, :]\n",
    "\n",
    "                    # Extract the log return feature from y_n\n",
    "                    r_n = y_n[:, :, 0]\n",
    "\n",
    "                    x_n_recon, r_n_recon, mu, log_var = model(x, y, x_c, y_c)\n",
    "\n",
    "                    # Compute the loss\n",
    "                    loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            # Print the losses for each epoch\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "        # Accumulate the validation loss for the current fold\n",
    "        total_val_loss += val_loss\n",
    "\n",
    "    # Calculate the average validation loss across all folds\n",
    "    avg_val_loss = total_val_loss / num_folds\n",
    "\n",
    "    # Check if the current hyperparameters yield a better validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_hyperparameters = params\n",
    "        print(best_hyperparameters)\n",
    "\n",
    "# Print the best hyperparameters and validation loss\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Train the final model with the best hyperparameters on the entire training set\n",
    "best_model = CVAE(input_size, best_hyperparameters['hidden_size'], best_hyperparameters['latent_size'],\n",
    "                  context_size, input_size, num_surfaces)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_hyperparameters['learning_rate'])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=best_hyperparameters['batch_size'], shuffle=True)\n",
    "\n",
    "for epoch in range(1, 1000):\n",
    "    # Training\n",
    "    best_model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        batch_size = x.shape[0]  # batch_size, context_length, num_surfaces, H, W = x.shape\n",
    "\n",
    "        # Generate a random day t to split the sequence into context and future\n",
    "        t = batch_size - num_surfaces - 1\n",
    "\n",
    "        x_c, x_n = x[:t, :], x[t:, :]\n",
    "        y_c, y_n = y[:t, :], y[t:, :]\n",
    "        r_n = y_n[:, :, 0]\n",
    "\n",
    "        x_n_recon, r_n_recon, mu, log_var = best_model(x, y, x_c, y_c)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(x_n[0], x_n_recon, r_n[0], r_n_recon, mu, log_var)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Print the training loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{best_hyperparameters['num_epochs']}], Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917006ec-bc27-460e-9991-91b3812a68ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a319b-980e-45e0-b72b-75f67a4e60fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd2f59-ffb0-4549-a60f-f80115909739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b40b4-9203-42f0-b78b-bfc44a66e0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d89bc-2bf1-4211-b601-397cd8a203df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5f90f-fe1d-496c-8feb-58929929fee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa169b-0bd2-4954-bdfa-8a2d7a70d408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eea76c-ffc8-42ee-99f4-882ca585a94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce3f06-9e3b-49b8-a2ac-0a87d41cb5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e94949-b7df-4e6c-8b89-a52f82092429",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Helper Function\n",
    "\n",
    "import time\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import norm\n",
    "\n",
    "moneyness_levels = [0.7, 0.85, 1, 1.15, 1.3]\n",
    "ttms = [1, 3, 6, 12, 24]\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def black_scholes_call(S, K, T, r, vol):\n",
    "    d1 = (np.log(S/K) + (r + 0.5*vol**2)*T) / (vol*np.sqrt(T))\n",
    "    d2 = d1 - vol * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - np.exp(-r * T) * K * norm.cdf(d2)\n",
    "\n",
    "def find_iv(market_price, S, K, T, r, max_iterations=100, precision=1e-6, lower_bound=0.1, upper_bound=0.3):\n",
    "    \"\"\"\n",
    "    Finds the implied volatility of a call option using the Black-Scholes model.\n",
    "\n",
    "    Parameters:\n",
    "        market_price (float): The market price of the call option.\n",
    "        S (float): The current price of the underlying asset.\n",
    "        K (float): The strike price of the option.\n",
    "        T (float): The time to expiration of the option (in years).\n",
    "        r (float): The risk-free interest rate.\n",
    "        max_iterations (int): The maximum number of iterations for the bisection method (default=100).\n",
    "        precision (float): The desired precision for the implied volatility (default=1e-6).\n",
    "        lower_bound (float): The lower bound of the implied volatility search range (default=0.01).\n",
    "        upper_bound (float): The upper bound of the implied volatility search range (default=0.7).\n",
    "\n",
    "    Returns:\n",
    "        float: The implied volatility of the call option.\n",
    "    \"\"\"\n",
    "    for i in range(max_iterations):\n",
    "        mid_point = (lower_bound + upper_bound) / 2\n",
    "        price = black_scholes_call(S, K, T, r, mid_point)\n",
    "        if abs(price - market_price) < precision:\n",
    "            return mid_point\n",
    "        elif price < market_price:\n",
    "            lower_bound = mid_point\n",
    "        else:\n",
    "            upper_bound = mid_point\n",
    "    return mid_point\n",
    "\n",
    "def find_closest_moneyness_range_and_ttm(underlying_price, strike_price, T):\n",
    "    moneyness = underlying_price / strike_price\n",
    "    #print(moneyness)\n",
    "    min_diff = float('inf')\n",
    "    #print(min_diff)\n",
    "    closest_moneyness_level = 99\n",
    "    moneyness_index = 99\n",
    "    ttm_index = 99\n",
    "    \n",
    "    counter = 0\n",
    "    for level in moneyness_levels:\n",
    "        diff = abs(moneyness - level)\n",
    "        #print(diff)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest_moneyness_level = level\n",
    "            moneyness_index = counter\n",
    "        counter = counter + 1\n",
    "\n",
    "    counter=0\n",
    "    for ttm in ttms:\n",
    "        if T == ttm:\n",
    "            ttm_index = counter\n",
    "        counter = counter + 1\n",
    "\n",
    "    return closest_moneyness_level, min_diff, moneyness_index, ttm_index\n",
    "\n",
    "\n",
    "def delta(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    delta = norm.cdf(d1)\n",
    "    return delta\n",
    "\n",
    "###In this generate empircal expected value/profit\n",
    "def predict_iv(x_c, y_c, t, N, moneyness_index, ttm_index):\n",
    "    total_profit = 0\n",
    "    num_iterations = 0\n",
    "\n",
    "    sum_iv = torch.zeros(5, 5)\n",
    "    num_iv = 0\n",
    "\n",
    "    timer = 0\n",
    "    start_time = time.time() \n",
    "    ##by the WLLN if our model is accurate it should converge to the true average profitability making a positive ev trading strategy\n",
    "    for i in range(1, N):\n",
    "        start_time = time.time() \n",
    "        x_n, r_n = model.generate(x_c, y_c, t)  # predict t units in the future\n",
    "\n",
    "        \n",
    "        predicted_iv_surface = x_n[0][0]\n",
    "\n",
    "        iv = predicted_iv_surface\n",
    "\n",
    "        sum_iv = sum_iv + abs(iv)\n",
    "        num_iv = num_iv + 1\n",
    "\n",
    "    \n",
    "    forecast_iv = (sum_iv / num_iv) #num_iv just N no\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    return forecast_iv, elapsed_time\n",
    "\n",
    "def check_call_option_loss(bought_options, t):\n",
    "    for option_trade in bought_options:\n",
    "        if option_trade['expiration_date'] == t:\n",
    "            #take the loss here, option is worthless remove it from the books - dont have to buy but take L  on option speculation\n",
    "            bought_options.remove(option_trade)\n",
    "\n",
    "    return bought_options\n",
    "\n",
    "def buy_underpriced_option(state, bought_options, expiration_date, strike_price, market_price, ttm_months, closest_moneyness_level):\n",
    "        if  state['cash_balance'] > market_price:\n",
    "            state['cash_balance'] -= market_price #we buy the option\n",
    "            #print(f\"option bought!!!\\nmarket_price: {market_price} || cash_balance: {state['cash_balance']}\")\n",
    "            bought_options.append({'expiration_date': expiration_date, 'market_price': market_price, 'strike_price': strike_price, 'TTM_months': ttm_months, 'moneyness_level': closest_moneyness_level})\n",
    "\n",
    "        return bought_options, state\n",
    "\n",
    "def check_for_profits(asset_price, market_price, bought_options, state,  ttm_months, closest_moneyness_level):\n",
    "        for option_trade in bought_options:\n",
    "            if market_price > option_trade['market_price'] and option_trade['TTM_months'] == ttm_months and option_trade['moneyness_level'] ==  closest_moneyness_level: #realised potential profit from mispricing assessment\n",
    "                state['cash_balance'] += market_price #we sell 100 units of the option at a profit\n",
    "                #print(f\"option sold for profit!!!\\nmarket_price: {market_price} || cash_balance: {state['cash_balance']}\")\n",
    "                bought_options.remove(option_trade)\n",
    "                \n",
    "        return bought_options, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5a06da8-d7cd-440c-84a9-823002b1ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (cnn): CNN(\n",
      "      (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=125, out_features=5, bias=True)\n",
      "    )\n",
      "    (mlp): Identity()\n",
      "    (lstm): LSTM(8, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (linear_mu): Linear(in_features=100, out_features=5, bias=True)\n",
      "    (linear_sigma): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      "  (context_encoder): ContextEncoder(\n",
      "    (cnn): CNN(\n",
      "      (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=125, out_features=5, bias=True)\n",
      "    )\n",
      "    (mlp): Identity()\n",
      "    (lstm): LSTM(8, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (linear): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(10, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (tcnn): TCNN(\n",
      "      (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
      "      (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (fc3): Linear(in_features=64, out_features=25, bias=True)\n",
      "    )\n",
      "    (mlp): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "(5975, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq, newton, bisect\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "import math\n",
    "\n",
    "extra_features_size = 3\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"Model loaded successfully.\")\n",
    "print(model.eval())\n",
    "\n",
    "# Underlying params for Black-Scholes\n",
    "r = 0.05  # risk-free rate\n",
    "longterm_volatility = 0.04  # long-term volatility - Heston set\n",
    "\n",
    "file_path = 'cvae_training_data.csv'\n",
    "iv_surfaces = pd.read_csv(file_path)\n",
    "\n",
    "df_orderbook_ts = 'orderbook_ts.csv'\n",
    "orderbook_ts = pd.read_csv('orderbook_ts.csv')\n",
    "\n",
    "# Extract IV surfaces and reshape them into 5x5 matrices\n",
    "X = np.array([np.fromstring(iv_surface[1:-1], sep=' ').reshape(5, 5) for iv_surface in df['iv_surface_flattened']])\n",
    "Y = df[['skew', 'slope', 'log_returns']].values\n",
    "\n",
    "# Normalize features\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Flatten the IV surfaces for scaling, then reshape back to (5, 5)\n",
    "X_flat = np.array([surface.flatten() for surface in X])\n",
    "x_reshaped = scaler_x.fit_transform(X_flat).reshape(-1, 5, 5)\n",
    "y_reshaped = scaler_y.fit_transform(Y).reshape(-1, 1, extra_features_size)\n",
    "\n",
    "iv_surfaces_tensor = TensorDataset(torch.Tensor(x_reshaped), torch.Tensor(y_reshaped))\n",
    "\n",
    "x = [x[0] for x in iv_surfaces_tensor]\n",
    "y = [x[1] for x in iv_surfaces_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885da774-b40e-46e8-bc6c-3c2b71bb59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq, newton, bisect\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"Model loaded successfully.\")\n",
    "print(model.eval())\n",
    "\n",
    "# Underlying params for Black-Scholes\n",
    "r = 0.05  # risk-free rate\n",
    "longterm_volatility = 0.04  # long-term volatility - Heston set\n",
    "\n",
    "file_path = 'option_prices_timeseries.csv'\n",
    "options = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "file_path = 'asset_prices.csv'\n",
    "asset_prices = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "file_path = 'combined_iv_data_small.csv'\n",
    "iv_surfaces = pd.read_csv(file_path, header=[0, 1], index_col=0)\n",
    "\n",
    "file_path = 'mean_iv_data_small.csv'\n",
    "mean_surface = pd.read_csv(file_path, index_col=0) #for bs\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "mean_surface_array = mean_surface.values\n",
    "\n",
    "print(mean_surface_array.shape)\n",
    "\n",
    "\n",
    "y = iv_surfaces[['Log Return', 'Skew', 'Slope']]\n",
    "x = iv_surfaces.drop(['Log Return', 'Skew', 'Slope'], axis=1)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "x_reshaped = x.reshape(x.shape[0], H, W)\n",
    "y_reshaped = y.reshape(y.shape[0], 1, extra_features_size)\n",
    "\n",
    "iv_surfaces_tensor = TensorDataset(torch.Tensor(x_reshaped), torch.Tensor(y_reshaped))\n",
    "\n",
    "x = [x[0] for x in iv_surfaces_tensor]\n",
    "y = [x[1] for x in iv_surfaces_tensor]\n",
    "\n",
    "sold_options_cvae = []\n",
    "sold_options_bs = []\n",
    "\n",
    "bought_options_cvae = []\n",
    "bought_options_bs = []\n",
    "\n",
    "N = 10\n",
    "\n",
    "# Track the value of the portfolios over time - each start with 1000 dollars\n",
    "cvae_portfolio_value_ts = [1000]\n",
    "bs_portfolio_value_ts = [1000]\n",
    "buy_and_hold_portfolio_value_ts = [1000]\n",
    "\n",
    "cvae_state = {'cash_balance': 1000, 'premiums_recieved': 0, 'stocks_held': 0, 'realised_losses': 0}\n",
    "bs_state = {'cash_balance': 1000, 'premiums_recieved': 0, 'stocks_held': 0, 'realised_losses': 0}\n",
    "hold_state = {'cash_balance': 1000, 'stocks_held': 0}\n",
    "\n",
    "initial_asset_price = options.loc[32][\"Asset Price\"].iloc[0]\n",
    "#print(f\"inital asset price is: {initial_asset_price}\")\n",
    "\n",
    "##all in on hold porfolio\n",
    "hold_state['stocks_held'] = math.floor(1000 / initial_asset_price)\n",
    "hold_state['cash_balance'] -= round(initial_asset_price * hold_state['stocks_held'], 2)\n",
    "\n",
    "#account for trading frictions loss of 5bp per trade\n",
    "start_t = 32\n",
    "end_t = 1032\n",
    "dates = [start_t - 1]\n",
    "\n",
    "cvae_times = []\n",
    "bs_times = []\n",
    "bs_iv_estimation_times = []\n",
    "forecast_times = []\n",
    "\n",
    "longterm_variance = 0.04 #long term average variance\n",
    "for t in range(start_t, end_t):\n",
    "    print(t)\n",
    "    dates.append(t)\n",
    "    #print(f\"cvae porfolio before day {t} of trading: {cvae_portfolio_value_ts[-1]}\")\n",
    "    \n",
    "    options_t = options.loc[t] #all options for timestep t\n",
    "    x_c = x[t-31:t]\n",
    "    y_c = y[t-31:t]\n",
    "\n",
    "    x_c = torch.stack(x_c)\n",
    "    y_c = torch.stack(y_c)\n",
    "    \n",
    "    S = options.loc[t][\"Asset Price\"].iloc[0]\n",
    "\n",
    "    ##assess if any bought options have lead to losses\n",
    "    bought_options_cvae = check_call_option_loss(bought_options_cvae, t)\n",
    "    bought_options_bs = check_call_option_loss(bought_options_bs, t)\n",
    "\n",
    "    ###Selling the call options and tracking them\n",
    "    for _, selected_row in options_t.iterrows():\n",
    "        T = selected_row[\"Time to Maturity (Months)\"]  # TTM in months\n",
    "        K = selected_row[\"Strike\"]\n",
    "        #print(K)\n",
    "        market_price_bid_call = selected_row[\"Bid Call Price\"]\n",
    "        market_price_ask_call = selected_row[\"Ask Call Price\"]\n",
    "        \n",
    "        expiration_date = t + int(T) * 30\n",
    "\n",
    "\n",
    "        iv_start_time = time.time()\n",
    "        iv = find_iv(market_price_bid_call, S, K, T / 12, r)\n",
    "        option_iv_estimation_time_taken = time.time() - iv_start_time\n",
    "\n",
    "\n",
    "        indexing_start_time = time.time()\n",
    "        closest_moneyness_level, diff, moneyness_index, ttm_index = find_closest_moneyness_range_and_ttm(S, K, int(T))\n",
    "        indexing_time_taken = time.time() - indexing_start_time\n",
    "        \n",
    "        #check if we found any profitable trades\n",
    "        cvae_buy_cycle_start_time = time.time()\n",
    "        bought_options_cvae, cvae_state = check_for_profits(market_price_ask_call, bought_options_cvae, cvae_state, int(T), closest_moneyness_level)\n",
    "        cvae_buy_cycle_time_taken = time.time() - cvae_buy_cycle_start_time\n",
    "\n",
    "        bs_buy_cycle_start_time = time.time()\n",
    "        bought_options_bs, bs_state = check_for_profits(market_price_ask_call, bought_options_bs, bs_state , int(T), closest_moneyness_level)\n",
    "        bs_buy_cycle_time_taken = time.time() - bs_buy_cycle_start_time\n",
    "        #sanity check\n",
    "        if iv == np.nan or moneyness_index == 99 or expiration_date > 6000:\n",
    "            continue\n",
    "\n",
    "        cvae_forecast_start_time = time.time()\n",
    "        forecast_iv, time_for_simulation = predict_iv(x_c, y_c, int(T), N, moneyness_index, ttm_index)\n",
    "\n",
    "        forecast_iv = forecast_iv.detach().numpy().reshape(-1, 25)\n",
    "        forecast_iv = scaler_x.inverse_transform(forecast_iv)\n",
    "        forecast_iv = forecast_iv.reshape(5,5)\n",
    "        cvae_forecast_time_taken = time.time() - cvae_forecast_start_time\n",
    "        #print(forecast_iv)\n",
    "        #print(f\"bs: {forecast_iv[moneyness_index][ttm_index]} || iv: {iv}\")\n",
    "\n",
    "        cvae_trade_start_time = time.time()\n",
    "        if diff < 0.1:\n",
    "            ##we buy underpriced options with the expectation the market will correct\n",
    "            if  abs(iv) > abs(forecast_iv[ttm_index][moneyness_index]):\n",
    "                #naked buying - then think about going delta neutral\n",
    "                bought_options_cvae, cvae_state = buy_underpriced_option(cvae_state, bought_options_cvae, expiration_date, market_price_bid_call, int(T), closest_moneyness_level)\n",
    "        cvae_trade_time_taken = time.time() - cvae_trade_start_time\n",
    "\n",
    "        bs_trade_start_time = time.time()\n",
    "        ##we buy underpriced options with the expectation the market will correct        \n",
    "        if abs(iv) > longterm_variance:\n",
    "            bought_options_bs, bs_state = buy_underpriced_option(bs_state, bought_options_bs, expiration_date, market_price_bid_call, int(T), closest_moneyness_level)\n",
    "        bs_trade_time_taken = time.time() - bs_trade_start_time\n",
    "\n",
    "        ##GOT ALL THE IMPORTANT TIMINGS FOR EACH NOW CALCULATE TIMINGS\n",
    "        bs_tt = option_iv_estimation_time_taken + indexing_time_taken + bs_buy_cycle_time_taken + bs_trade_time_taken\n",
    "        bs_times.append(bs_tt)\n",
    "    \n",
    "        cvae_tt = option_iv_estimation_time_taken + indexing_time_taken + cvae_buy_cycle_time_taken + cvae_forecast_time_taken + cvae_trade_time_taken\n",
    "        cvae_times.append(cvae_tt)\n",
    "\n",
    "        forecast_times.append(cvae_forecast_time_taken)\n",
    "        bs_iv_estimation_times.append(option_iv_estimation_time_taken)\n",
    "\n",
    "    \n",
    "    ###at the end of the day of trading calculate porfolio values in ts\n",
    "    cvae_portfolio_value = cvae_state['cash_balance'] + S * cvae_state['stocks_held']\n",
    "    cvae_portfolio_value_ts.append(cvae_portfolio_value)\n",
    "\n",
    "    ###at the end of the day of trading calculate porfolio values in ts\n",
    "    bs_portfolio_value = bs_state['cash_balance'] + S * bs_state['stocks_held']\n",
    "    bs_portfolio_value_ts.append(bs_portfolio_value)\n",
    "\n",
    "    buy_and_hold_portfolio_value = hold_state['cash_balance'] + S * hold_state['stocks_held']\n",
    "    buy_and_hold_portfolio_value_ts.append(buy_and_hold_portfolio_value)\n",
    "\n",
    "# Normalize the portfolio values to start at 1\n",
    "cvae_portfolio_value_ts_norm = np.array(cvae_portfolio_value_ts) / cvae_portfolio_value_ts[0]\n",
    "bs_portfolio_value_ts_norm = np.array(bs_portfolio_value_ts) / bs_portfolio_value_ts[0]\n",
    "buy_and_hold_portfolio_value_ts_norm = np.array(buy_and_hold_portfolio_value_ts) / buy_and_hold_portfolio_value_ts[0]\n",
    "\n",
    "plt.figure(figsize=(10, 12))  # Increased vertical size to accommodate two full-sized plots\n",
    "\n",
    "# Plot for CVAE and Black-Scholes Portfolios\n",
    "plt.subplot(2, 1, 1)  # 2 rows, 1 column, 1st subplot\n",
    "plt.plot(dates, cvae_portfolio_value_ts_norm, label='CVAE Volatility Arbitrage Portfolio', color='blue')\n",
    "plt.plot(dates, bs_portfolio_value_ts_norm, label='Black-Scholes Volatility Arbitrage Portfolio', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Portfolio Value')\n",
    "plt.title('CVAE and Black-Scholes Portfolio Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot for Buy and Hold Portfolio\n",
    "plt.subplot(2, 1, 2)  # 2 rows, 1 column, 2nd subplot\n",
    "plt.plot(dates, buy_and_hold_portfolio_value_ts_norm, label='Buy and Hold Portfolio', color='green')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Portfolio Value')\n",
    "plt.title('Buy and Hold Portfolio Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to fit everything nicely\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean of each list\n",
    "cvae_mean = np.mean(cvae_times)\n",
    "bs_mean = np.mean(bs_times)\n",
    "bs_iv_estimation_mean = np.mean(bs_iv_estimation_times)\n",
    "forecast_mean = np.mean(forecast_times)\n",
    "\n",
    "print(f\"mean cvae time taken for trade: {cvae_mean}\")\n",
    "print(f\"mean bs benchmark time taken for trade: {bs_mean}\")\n",
    "print(f\"mean time taken to calculate iv of option using BS: {bs_iv_estimation_mean}\")\n",
    "print(f\"mean time taken to forecast iv surface using CVAE: {forecast_mean}\")\n",
    "\n",
    "# Create labels and values for the bar chart\n",
    "labels = ['CVAE', 'BS', 'BS IV Estimation', 'Forecast']\n",
    "means = [cvae_mean, bs_mean, bs_iv_estimation_mean, forecast_mean]\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))  # Set figure size\n",
    "plt.bar(labels, means, color=['blue', 'red', 'green', 'purple'])  # Plot bars with different colors\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Average Time (s)')\n",
    "plt.title('Average Execution Times')\n",
    "plt.ylim(0, max(means) + 0.05)  # Set y-limit to give some space above the tallest bar\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
