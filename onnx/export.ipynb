{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "245ca46e-748d-40ed-8882-242aa2ca1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CNN and MLP primitive\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##The encoder in the paper takes x in R^(TxHxW) and y in R^(TxE) and maps them to R^(TxL) the dimensionality of the context encoder\n",
    "##and encoder is set to 5. So this cnn needs to output (32x5) so each (surface in R^(5x5) -> (z in R^5)\n",
    "\n",
    "##we enhance the dimensionality of the iv surface first by upgrading the number of channels. The reasoning behind this is similar to why \n",
    "##we do this in transformer architecture. A larger dimensional space will be able to capture more nuanced information and represent it in number form\n",
    "##then we compress this to something digestable\n",
    "class CNN(nn.Module):\n",
    "    #input_size and output_size represent the number of channels in the input and output data\n",
    "    #channels is the number of dimensions a single data point will have ie RGB = 3 channelss\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(output_size * 5 * 5, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(batch_size, 1, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(f\"x shape before resize and convultion passthrough is {x.shape}\")\n",
    "        #x = x.reshape(batch_size, 1, 5)\n",
    "        x = torch.reshape(x, (batch_size, -1))\n",
    "        x = self.fc(x)\n",
    "        #print(f\"after passthrough into convultion layers and fully connected layer ther shape of x is {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = F.relu(self.fc1(y))\n",
    "\n",
    "class TCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_surfaces):\n",
    "        super(TCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5 * 5 * num_surfaces)\n",
    "        self.output_size = output_size\n",
    "        self.num_surfaces = num_surfaces\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.reshape(x, (-1, self.num_surfaces, 5, 5))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c059936-c0ee-480b-b3ea-649b693073b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ENCODER DECODER CONTEXTENCODER\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cnn = CNN(input_size, 5)\n",
    "        self.mlp = nn.Identity()\n",
    "        self.lstm = nn.LSTM(5 + 3, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.linear_sigma = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_encoded = self.cnn(x)\n",
    "        y_encoded = self.mlp(y)\n",
    "        y_encoded = torch.squeeze(y_encoded, dim=1)\n",
    "        #print(f\"x_encoded shape is {x_encoded.shape}  y_encoded shape is {y_encoded.shape}\")\n",
    "        encoded = torch.cat((x_encoded, y_encoded), dim=-1)\n",
    "        #print(f\"concatenated vector is of size {encoded.shape}\")\n",
    "        _, (hidden, _) = self.lstm(encoded)\n",
    "        #print(\"hidden state created\")\n",
    "        hidden = hidden[-1]  # Take the last hidden state\n",
    "        mu = self.linear_mu(hidden)\n",
    "        log_var = self.linear_sigma(hidden)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        #print(\"encoding successful\")\n",
    "        return z, mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, context_size):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.cnn = CNN(input_size, 5)\n",
    "        self.mlp = nn.Identity()\n",
    "        self.lstm = nn.LSTM(5 + 3, hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, context_size)\n",
    "\n",
    "    def forward(self, x_c, y_c):\n",
    "        x_encoded = self.cnn(x_c)\n",
    "        y_encoded = self.mlp(y_c)\n",
    "        y_encoded = torch.squeeze(y_encoded, dim=1)\n",
    "        #print(f\"x_encoded size = {x_encoded.shape} ||y_encoded size = {y_encoded.shape}\")\n",
    "        encoded = torch.cat((x_encoded, y_encoded), dim=-1)\n",
    "        _, (hidden, _) = self.lstm(encoded)\n",
    "        hidden = hidden[-1]  # Take the last hidden state\n",
    "        zeta = self.linear(hidden)\n",
    "        #print(\"context encoding successful\")\n",
    "        return zeta\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_surfaces):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_size + 5, hidden_size, num_layers=2, batch_first=True, dropout=0.2) #latent\n",
    "        self.tcnn = TCNN(hidden_size, output_size, num_surfaces)\n",
    "        self.mlp = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, z, zeta):\n",
    "        # Reshape z and zeta to have shape (1, 5)\n",
    "        z = torch.reshape(z, (1, -1))\n",
    "        zeta = torch.reshape(zeta, (1, -1))\n",
    "        # Concatenate z and zeta along the second dimension to get shape (1, 10)\n",
    "        z_concat = torch.cat((z, zeta), dim=1)\n",
    "        \n",
    "        #print(f\"z_concat is of size {z_concat.shape}\")\n",
    "        hidden, _ = self.lstm(z_concat)\n",
    "        #print('i am here')\n",
    "        #print(f\"hidden state published, hidden state shape {hidden.shape}\")\n",
    "        x_n = self.tcnn(hidden) \n",
    "        #print('i am here')\n",
    "        r_n = self.mlp(hidden)\n",
    "        #print(\"decoding successful\")\n",
    "        return torch.squeeze(x_n, dim=0), torch.squeeze(r_n, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5ca9fc2-8945-41d2-bcad-daee0842e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (cnn): CNN(\n",
      "      (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=125, out_features=5, bias=True)\n",
      "    )\n",
      "    (mlp): Identity()\n",
      "    (lstm): LSTM(8, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (linear_mu): Linear(in_features=100, out_features=5, bias=True)\n",
      "    (linear_sigma): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      "  (context_encoder): ContextEncoder(\n",
      "    (cnn): CNN(\n",
      "      (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=125, out_features=5, bias=True)\n",
      "    )\n",
      "    (mlp): Identity()\n",
      "    (lstm): LSTM(8, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (linear): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(10, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (tcnn): TCNN(\n",
      "      (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
      "      (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (fc3): Linear(in_features=64, out_features=25, bias=True)\n",
      "    )\n",
      "    (mlp): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 100\n",
    "latent_size = 5\n",
    "context_size = 5\n",
    "output_size = 1\n",
    "num_surfaces = 1\n",
    "ttm = 10  # Number of time steps to generate\n",
    "\n",
    "batch_size = 31\n",
    "\n",
    "model = CVAE(input_size, hidden_size, latent_size, context_size, input_size, num_surfaces)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "print(model.eval())\n",
    "\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder\n",
    "context_encoder = model.context_encoder\n",
    "\n",
    "batch_size = 31\n",
    "\n",
    "# Create dummy inputs for the ONNX export\n",
    "x = torch.randn(batch_size, 1, 5, 5)\n",
    "y = torch.randn(batch_size, 1, 3)\n",
    "\n",
    "# Export the encoder to ONNX\n",
    "torch.onnx.export(encoder, (x, y), \"encoder.onnx\", opset_version=11, input_names=['x', 'y'], output_names=['z', 'mu', 'log_var'])\n",
    "\n",
    "# Create dummy inputs for the ONNX export\n",
    "z = torch.randn(1, latent_size)\n",
    "zeta = torch.randn(1, context_size)\n",
    "\n",
    "# Export the decoder to ONNX\n",
    "torch.onnx.export(decoder, (z, zeta), \"decoder.onnx\", opset_version=11, input_names=['z', 'zeta'], output_names=['x_n', 'r_n'])\n",
    "\n",
    "# Create dummy inputs for the ONNX export\n",
    "x_c = torch.randn(batch_size, 5, 5)\n",
    "y_c = torch.randn(batch_size, 3)\n",
    "\n",
    "# Export the context encoder to ONNX\n",
    "torch.onnx.export(context_encoder, (x_c, y_c), \"context_encoder.onnx\", opset_version=11, input_names=['x_c', 'y_c'], output_names=['zeta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ed3f4-351f-4c9f-ae7f-7c16551e0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "example_inputs = (x, y)\n",
    "\n",
    "##static quantisation\n",
    "encoder_to_quantise = copy.deepcopy(encoder)\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n",
    "encoder_to_quantise.eval()\n",
    "# prepare\n",
    "encoder_prepared = quantize_fx.prepare_fx(encoder_to_quantise, qconfig_mapping, example_inputs)\n",
    "# calibrate (not shown)\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(encoder_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c5b51-56e5-409c-8e3e-62382314050f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
